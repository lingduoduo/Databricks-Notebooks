{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.7\n  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\nCollecting langgraph==0.5.3\n  Downloading langgraph-0.5.3-py3-none-any.whl.metadata (6.9 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (6.0)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.7)\n  Downloading sqlalchemy-2.0.45-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.3.7)\n  Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\nCollecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n  Downloading langchain_core-0.3.83-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.7)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (1.23.5)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain==0.3.7)\n  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (8.2.2)\nCollecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph==0.5.3)\n  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\nCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph==0.5.3)\n  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.5.3)\n  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\nCollecting xxhash>=3.5.0 (from langgraph==0.5.3)\n  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain_community\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.7)\n  Downloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting databricks-vectorsearch<0.41,>=0.40 (from langchain-databricks)\n  Downloading databricks_vectorsearch-0.40-py3-none-any.whl.metadata (2.8 kB)\nCollecting mlflow>=2.9 (from langchain-databricks)\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting numpy<2,>=1 (from langchain==0.3.7)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from langchain-databricks) (1.11.1)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading frozenlist-1.8.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.11.4)\nCollecting protobuf<5,>=3.12.0 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\nCollecting deprecation>=2 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n  Downloading langchain_core-0.3.82-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.80-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.77-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\nINFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (4.10.0)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\nCollecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading orjson-3.11.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting pip>=25.2 (from langchain-text-splitters<0.4.0,>=0.3.0->langchain==0.3.7)\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.5.3)\n  Downloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.2 kB)\nCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph==0.5.3)\n  Downloading langgraph_prebuilt-0.5.1-py3-none-any.whl.metadata (4.5 kB)\nINFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\nCollecting mlflow>=2.9 (from langchain-databricks)\n  Downloading mlflow-3.8.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.5.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\nINFO: pip is still looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.3-py3-none-any.whl.metadata (29 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\nCollecting Flask<4 (from mlflow>=2.9->langchain-databricks)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting Jinja2<4,>=2.11 (from mlflow>=2.9->langchain-databricks)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow>=2.9->langchain-databricks)\n  Downloading alembic-1.18.1-py3-none-any.whl.metadata (7.2 kB)\nCollecting docker<8,>=4.0.0 (from mlflow>=2.9->langchain-databricks)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow>=2.9->langchain-databricks)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow>=2.9->langchain-databricks)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting markdown<4,>=3.3 (from mlflow>=2.9->langchain-databricks)\n  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (3.7.2)\nRequirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (1.5.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (14.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (1.3.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.0.4)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.40.0)\nCollecting fastapi<1 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.0.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.1)\nCollecting uvicorn<1 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\nCollecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.3 kB)\nCollecting typing-extensions>=4.7 (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (2023.7.22)\nCollecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.3.7)\n  Downloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.9->langchain-databricks)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting click<9,>=7.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (2.7 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.9->langchain-databricks)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.9->langchain-databricks)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow>=2.9->langchain-databricks) (2.8.2)\nCollecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas!=2.3.0,<3->mlflow>=2.9->langchain-databricks) (2022.7)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.9->langchain-databricks) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.9->langchain-databricks) (2.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.35.0)\nCollecting starlette<0.51.0,>=0.40.0 (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\nCollecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.11.0)\nCollecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.9->langchain-databricks) (1.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\nDownloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m23.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langgraph-0.5.3-py3-none-any.whl (143 kB)\nDownloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m68.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_databricks-0.1.2-py3-none-any.whl (21 kB)\nDownloading aiohttp-3.13.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m69.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.40-py3-none-any.whl (12 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\nDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\nDownloading langgraph_prebuilt-0.5.1-py3-none-any.whl (23 kB)\nDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\nDownloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/29.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m28.8/29.0 MB\u001B[0m \u001B[31m220.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.0/29.0 MB\u001B[0m \u001B[31m137.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m127.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/14.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.2/14.2 MB\u001B[0m \u001B[31m153.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\nDownloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m84.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m67.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (213 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading alembic-1.18.1-py3-none-any.whl (260 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading frozenlist-1.8.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (233 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (577 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/577.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m577.1/577.1 kB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading markdown-3.10-py3-none-any.whl (107 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading multidict-6.7.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (246 kB)\nDownloading orjson-3.11.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\nDownloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (202 kB)\nDownloading propcache-0.4.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (214 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl (294 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading yarl-1.22.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (368 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading click-8.3.1-py3-none-any.whl (108 kB)\nDownloading fastapi-0.128.0-py3-none-any.whl (103 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (24 kB)\nDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\nDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\nDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\nDownloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading starlette-0.50.0-py3-none-any.whl (74 kB)\nInstalling collected packages: xxhash, typing-extensions, python-dotenv, protobuf, propcache, ormsgpack, orjson, numpy, multidict, marshmallow, markupsafe, markdown, jsonpointer, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, click, blinker, attrs, annotated-types, annotated-doc, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic-core, opentelemetry-api, Mako, jsonpatch, Jinja2, httpcore, graphql-relay, docker, anyio, aiosignal, starlette, pydantic, opentelemetry-semantic-conventions, httpx, graphene, Flask, dataclasses-json, alembic, aiohttp, pydantic-settings, opentelemetry-sdk, langsmith, langgraph-sdk, fastapi, mlflow-skinny, langchain-core, mlflow, langgraph-checkpoint, langchain-text-splitters, databricks-vectorsearch, langgraph-prebuilt, langchain-databricks, langchain, langgraph, langchain_community\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.3\n    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'click'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.4\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.11.4\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-f5ca8936-66ae-4563-905a-e50bfc97dcb6\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.69.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed Flask-3.1.2 Jinja2-3.1.6 Mako-1.3.10 SQLAlchemy-2.0.35 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 alembic-1.18.1 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.12.1 attrs-25.4.0 blinker-1.9.0 click-8.3.1 databricks-vectorsearch-0.40 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 fastapi-0.128.0 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 itsdangerous-2.2.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.7 langchain-core-0.3.63 langchain-databricks-0.1.2 langchain-text-splitters-0.3.8 langchain_community-0.3.7 langgraph-0.5.3 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.5.1 langgraph-sdk-0.1.74 langsmith-0.1.147 markdown-3.10 markupsafe-3.0.3 marshmallow-3.26.2 mlflow-2.22.4 mlflow-skinny-2.22.4 multidict-6.7.0 numpy-1.26.4 opentelemetry-api-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 orjson-3.11.5 ormsgpack-1.12.1 propcache-0.4.1 protobuf-4.25.8 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 requests-toolbelt-1.0.0 starlette-0.50.0 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 uvicorn-0.40.0 werkzeug-3.1.5 xxhash-3.6.0 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "%pip install -U langchain==0.3.7 langgraph==0.5.3 langchain_community langchain-databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c57fc7-083e-4750-87ef-2391de68d4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Databricks LLM configured: databricks-meta-llama-3-1-8b-instruct\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-f5ca8936-66ae-4563-905a-e5/.ipykernel/1785747/command-1575010149449533-1997263450:12: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Databricks setup + imports + LLM config (no OpenAI)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_ENDPOINT_NAME,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(\"✅ Databricks LLM configured:\", LLM_ENDPOINT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d62a2a-63cf-4895-9ff4-6adfd84568bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helpers ready: query_model(), extract_json_from_review_text()\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Databricks-native LLM query helper + robust JSON extraction\n",
    "\n",
    "def query_model(system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Databricks-native chat call using ChatDatabricks.\n",
    "    Returns the raw text content.\n",
    "    \"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ])\n",
    "    return (resp.content or \"\").strip()\n",
    "\n",
    "\n",
    "def extract_json_from_review_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Tries to parse the JSON inside the model's response.\n",
    "    The prompt asks for:\n",
    "      THOUGHT: ...\n",
    "      REVIEW JSON:\n",
    "      ```json\n",
    "      { ... }\n",
    "      ```\n",
    "    We robustly extract the first JSON object found.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Prefer fenced ```json blocks\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Fallback: take the first {...} span\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = text[start:end+1]\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"✅ Helpers ready: query_model(), extract_json_from_review_text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844af4b8-e6a9-4159-a7c1-60d04a4964b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# LLM QUERY + JSON EXTRACTION\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def query_model(system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Databricks-native chat call.\"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ])\n",
    "    return (resp.content or \"\").strip()\n",
    "\n",
    "\n",
    "def extract_json_from_review_text(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract JSON block from reviewer output.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Prefer fenced JSON\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Fallback: first {...}\n",
    "    start, end = text.find(\"{\"), text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        try:\n",
    "            return json.loads(text[start:end + 1])\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e34c9c5-d8ef-41c2-a2e3-1df0f6e539b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# SCORING PROMPT\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "TEMPLATE_INSTRUCTIONS = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "THOUGHT:\n",
    "<THOUGHT>\n",
    "\n",
    "REVIEW JSON:\n",
    "```json\n",
    "<JSON>\n",
    "```\n",
    "\n",
    "In <JSON>, include the following fields exactly:\n",
    "- Summary\n",
    "- Strengths\n",
    "- Weaknesses\n",
    "- Originality (1–4)\n",
    "- Quality (1–4)\n",
    "- Clarity (1–4)\n",
    "- Significance (1–4)\n",
    "- Questions\n",
    "- Limitations\n",
    "- Ethical Concerns (boolean)\n",
    "- Soundness (1–4)\n",
    "- Presentation (1–4)\n",
    "- Contribution (1–4)\n",
    "- Overall (1–10)\n",
    "- Confidence (1–5)\n",
    "- Decision (Accept or Reject)\n",
    "\n",
    "Return ONLY this format.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acdb6e8-3fa4-4241-a879-e32d534f740a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# CORE SCORING FUNCTION\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def get_score(\n",
    "    outlined_plan: str,\n",
    "    latex: str,\n",
    "    reward_model_llm: str = None,\n",
    "    reviewer_type: Optional[str] = None,\n",
    "    attempts: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "    last_error = None\n",
    "\n",
    "    for _ in range(attempts):\n",
    "        sys_prompt = (reviewer_type or \"You are a harsh but fair reviewer.\") + \"\\n\" + TEMPLATE_INSTRUCTIONS\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "OUTLINED PLAN:\n",
    "{outlined_plan}\n",
    "\n",
    "LATEX REPORT:\n",
    "{latex}\n",
    "\"\"\".strip()\n",
    "\n",
    "        raw = query_model(sys_prompt, user_prompt)\n",
    "        parsed = extract_json_from_review_text(raw)\n",
    "\n",
    "        if parsed is not None:\n",
    "            return {\n",
    "                \"raw_text\": raw,\n",
    "                \"review_json\": parsed,\n",
    "            }\n",
    "\n",
    "        last_error = \"JSON parsing failed\"\n",
    "\n",
    "    return {\n",
    "        \"raw_text\": \"\",\n",
    "        \"review_json\": None,\n",
    "        \"error\": last_error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a8e8d8-9b47-4c70-8614-bb4318eafa1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# REVIEWERS AGENT\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "class ReviewersAgent:\n",
    "    def __init__(self, model: str = \"databricks\", notes: Optional[List[str]] = None):\n",
    "        self.notes = notes or []\n",
    "        self.model = model\n",
    "\n",
    "    def inference(self, plan: str, report: str) -> str:\n",
    "        reviewer_1 = \"You are a harsh but fair reviewer and expect good experiments that lead to insights.\"\n",
    "        reviewer_2 = \"You are a harsh and critical but fair reviewer looking for impactful ideas.\"\n",
    "        reviewer_3 = \"You are a harsh but fair open-minded reviewer seeking novel ideas.\"\n",
    "\n",
    "        r1 = get_score(plan, report, reviewer_type=reviewer_1)\n",
    "        r2 = get_score(plan, report, reviewer_type=reviewer_2)\n",
    "        r3 = get_score(plan, report, reviewer_type=reviewer_3)\n",
    "\n",
    "        return (\n",
    "            f\"Reviewer #1:\\n{json.dumps(r1['review_json'], indent=2)}\\n\\n\"\n",
    "            f\"Reviewer #2:\\n{json.dumps(r2['review_json'], indent=2)}\\n\\n\"\n",
    "            f\"Reviewer #3:\\n{json.dumps(r3['review_json'], indent=2)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373b87d7-63b6-49da-8ee0-6adcd6ac70b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewer #1:\n{\n  \"Summary\": \"This paper evaluates various transformer pruning techniques, but the introduction lacks context and the report is incomplete.\",\n  \"Strengths\": [\n    \"Clear research question\",\n    \"Good selection of pruning techniques\"\n  ],\n  \"Weaknesses\": [\n    \"Brief and lacking introduction\",\n    \"Incomplete report\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 3,\n  \"Clarity\": 2,\n  \"Significance\": 2,\n  \"Questions\": [\n    \"What are the specific research questions being addressed?\",\n    \"Why are these pruning techniques being evaluated?\"\n  ],\n  \"Limitations\": [\n    \"Incomplete report\",\n    \"Lack of context in introduction\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 2,\n  \"Presentation\": 2,\n  \"Contribution\": 2,\n  \"Overall\": 6,\n  \"Confidence\": 3,\n  \"Decision\": \"Reject\"\n}\n\nReviewer #2:\n{\n  \"Summary\": \"The paper explores transformer pruning techniques, but the evaluation aspect is unclear.\",\n  \"Strengths\": [\n    \"Clear introduction to transformer pruning techniques\",\n    \"Potential for in-depth analysis\"\n  ],\n  \"Weaknesses\": [\n    \"Lack of clarity on evaluation goals and methods\",\n    \"Unclear scope of the paper\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 2,\n  \"Clarity\": 3,\n  \"Significance\": 2,\n  \"Questions\": [\n    \"What are the specific evaluation goals and methods?\",\n    \"How does this paper contribute to the field of transformer pruning?\"\n  ],\n  \"Limitations\": [\n    \"Unclear evaluation scope\",\n    \"Potential for biased or incomplete analysis\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 2,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Overall\": 6,\n  \"Confidence\": 3,\n  \"Decision\": \"Reject\"\n}\n\nReviewer #3:\n{\n  \"Summary\": \"This paper evaluates transformer pruning techniques, but the report is incomplete and lacks a clear research question.\",\n  \"Strengths\": [\n    \"Well-defined research topic\",\n    \"Potential for comprehensive comparison of pruning methods\"\n  ],\n  \"Weaknesses\": [\n    \"Incomplete report\",\n    \"Lack of clear research question\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 2,\n  \"Clarity\": 2,\n  \"Significance\": 2,\n  \"Questions\": [\n    \"What specific pruning techniques will be evaluated?\",\n    \"How will the authors compare the results?\"\n  ],\n  \"Limitations\": [\n    \"Incomplete report\",\n    \"Lack of clear research question\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 2,\n  \"Presentation\": 2,\n  \"Contribution\": 2,\n  \"Overall\": 6,\n  \"Confidence\": 3,\n  \"Decision\": \"Reject\"\n}\n"
     ]
    }
   ],
   "source": [
    "agent = ReviewersAgent()\n",
    "demo_plan = \"Evaluate transformer pruning techniques.\"\n",
    "demo_report = \"\\\\section{Introduction} This paper explores...\"\n",
    "\n",
    "print(agent.inference(demo_plan, demo_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d13a68-c11e-4ac6-a347-a0e9c3f57dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Databricks LLM configured: databricks-meta-llama-3-1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Setup: imports + Databricks LLM config (AgentLaboratory-style backend)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_ENDPOINT_NAME,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(\"✅ Databricks LLM configured:\", LLM_ENDPOINT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebfa3ef-0c96-4507-9d03-658c4e660a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helpers ready: query_model(), extract_first_json_object()\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Core helpers: query_model + JSON extraction (used by all agents)\n",
    "\n",
    "def query_model(system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Databricks-native LLM call.\"\"\"\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ])\n",
    "    return (resp.content or \"\").strip()\n",
    "\n",
    "\n",
    "def extract_first_json_object(text: str) -> Optional[Dict[str, Any]]:\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # 1) Prefer fenced JSON blocks: ```json ... ```\n",
    "    start_tag = \"```json\"\n",
    "    end_tag = \"```\"\n",
    "\n",
    "    start = text.find(start_tag)\n",
    "    if start != -1:\n",
    "        # Find the closing fence after the start fence\n",
    "        fence_start_end = text.find(\"\\n\", start)\n",
    "        if fence_start_end != -1:\n",
    "            end = text.find(end_tag, fence_start_end)\n",
    "            if end != -1:\n",
    "                block = text[fence_start_end:end].strip()\n",
    "                # Some models include leading/trailing text; try to isolate {...}\n",
    "                s = block.find(\"{\")\n",
    "                e = block.rfind(\"}\")\n",
    "                if s != -1 and e != -1 and e > s:\n",
    "                    try:\n",
    "                        return json.loads(block[s:e+1])\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "    # 2) Fallback: first {...} in the whole text\n",
    "    s = text.find(\"{\")\n",
    "    e = text.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        try:\n",
    "            return json.loads(text[s:e+1])\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"✅ Helpers ready: query_model(), extract_first_json_object()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ac4374-2544-4c15-a327-f962600fd439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# REVIEWER PROMPT\n",
    "# -----------------------------\n",
    "\n",
    "TEMPLATE_INSTRUCTIONS = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "THOUGHT:\n",
    "<THOUGHT>\n",
    "\n",
    "REVIEW JSON:\n",
    "```json\n",
    "<JSON>\n",
    "```\n",
    "\n",
    "In <THOUGHT>, briefly discuss your reasoning specific to this paper.\n",
    "\n",
    "In <JSON>, provide the review in JSON format with the following fields in the order:\n",
    "- Summary\n",
    "- Strengths\n",
    "- Weaknesses\n",
    "- Originality (1-4)\n",
    "- Quality (1-4)\n",
    "- Clarity (1-4)\n",
    "- Significance (1-4)\n",
    "- Questions\n",
    "- Limitations\n",
    "- Ethical Concerns (boolean)\n",
    "- Soundness (1-4)\n",
    "- Presentation (1-4)\n",
    "- Contribution (1-4)\n",
    "- Overall (1-10)\n",
    "- Confidence (1-5)\n",
    "- Decision (Accept or Reject)\n",
    "\n",
    "Return ONLY the required format. The JSON must be valid.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b7282a-551a-4ac5-8704-dbf9b2c3fe0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_score(\n",
    "    outlined_plan: str,\n",
    "    latex: str,\n",
    "    reviewer_type: str,\n",
    "    attempts: int = 2,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run a reviewer scoring pass.\"\"\"\n",
    "    last_error = None\n",
    "\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        system_prompt = f\"{reviewer_type}\\n\\n{TEMPLATE_INSTRUCTIONS}\"\n",
    "        user_prompt = f\"\"\"\n",
    "OUTLINED PLAN:\n",
    "{outlined_plan}\n",
    "\n",
    "LATEX REPORT:\n",
    "{latex}\n",
    "\"\"\".strip()\n",
    "\n",
    "        raw = query_model(system_prompt, user_prompt)\n",
    "        parsed = extract_first_json_object(raw)\n",
    "\n",
    "        if parsed is not None:\n",
    "            return {\n",
    "                \"raw_text\": raw,\n",
    "                \"review_json\": parsed,\n",
    "            }\n",
    "\n",
    "        last_error = f\"Attempt {attempt}: failed to parse JSON\"\n",
    "\n",
    "    return {\n",
    "        \"raw_text\": \"\",\n",
    "        \"review_json\": None,\n",
    "        \"error\": last_error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f7aaab-dcd8-4d4d-b9bf-24fbc056b600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Summary': 'This paper explores pruning methods for transformers, but the introduction is too generic and lacks concrete information.', 'Strengths': ['Well-structured outline', 'Interesting topic'], 'Weaknesses': ['Lack of concrete information in the introduction', 'Unclear methodology'], 'Originality': 2, 'Quality': 2, 'Clarity': 3, 'Significance': 2, 'Questions': ['What specific pruning methods are explored?', 'How are the results evaluated?'], 'Limitations': ['Insufficient information in the introduction', 'Unclear methodology'], 'Ethical Concerns': False, 'Soundness': 2, 'Presentation': 3, 'Contribution': 2, 'Overall': 6, 'Confidence': 3, 'Decision': 'Major Revisions Required'}\n"
     ]
    }
   ],
   "source": [
    "plan = \"Evaluate pruning methods for transformers.\"\n",
    "report = \"\\\\section{Introduction} This paper explores...\"\n",
    "review = get_score(\n",
    "    outlined_plan=plan,\n",
    "    latex=report,\n",
    "    reviewer_type=\"You are a harsh but fair reviewer.\"\n",
    ")\n",
    "print(review[\"review_json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cd4691-54e1-46c5-a814-1d54fbc4217b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ReviewersAgent ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — ReviewersAgent (3 reviewers) using reviewer_scoring.get_score\n",
    "\n",
    "import json\n",
    "class ReviewersAgent:\n",
    "    def __init__(self, notes=None):\n",
    "        self.notes = notes or []\n",
    "\n",
    "    def inference(self, plan: str, report: str) -> str:\n",
    "        reviewer_1 = \"You are a harsh but fair reviewer and expect good experiments that lead to insights for the research topic.\"\n",
    "        reviewer_2 = \"You are a harsh and critical but fair reviewer who is looking for an idea that would be impactful in the field.\"\n",
    "        reviewer_3 = \"You are a harsh but fair open-minded reviewer that is looking for novel ideas that have not been proposed before.\"\n",
    "\n",
    "        r1 = get_score(outlined_plan=plan, latex=report, reviewer_type=reviewer_1, attempts=2)\n",
    "        r2 = get_score(outlined_plan=plan, latex=report, reviewer_type=reviewer_2, attempts=2)\n",
    "        r3 = get_score(outlined_plan=plan, latex=report, reviewer_type=reviewer_3, attempts=2)\n",
    "\n",
    "        def fmt(label, r):\n",
    "            if r.get(\"review_json\") is None:\n",
    "                return f\"{label}:\\n❌ No JSON parsed. Error: {r.get('error')}\\nRaw:\\n{r.get('raw_text','')}\"\n",
    "            return f\"{label}:\\n{json.dumps(r['review_json'], indent=2)}\"\n",
    "\n",
    "        return \"\\n\\n\".join([\n",
    "            fmt(\"Reviewer #1\", r1),\n",
    "            fmt(\"Reviewer #2\", r2),\n",
    "            fmt(\"Reviewer #3\", r3),\n",
    "        ])\n",
    "\n",
    "\n",
    "reviewers_agent = ReviewersAgent()\n",
    "print(\"✅ ReviewersAgent ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26489ba2-67bd-453f-bb77-00c66bd4e091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reviewer Feedback ===\nReviewer #1:\n{\n  \"Summary\": \"This paper proposes to study pruning strategies for transformer-based language models to reduce inference cost while preserving downstream task accuracy. However, the proposal lacks concrete details and a clear direction.\",\n  \"Strengths\": [\n    \"Exploring pruning methods for transformer models is a timely and relevant research topic\"\n  ],\n  \"Weaknesses\": [\n    \"Lack of concrete details about experiments, evaluation metrics, and analysis\",\n    \"Introduction is too brief and lacks context\",\n    \"Method section is too concise\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 2,\n  \"Clarity\": 2,\n  \"Significance\": 2,\n  \"Questions\": [\n    \"What are the specific pruning strategies being evaluated?\",\n    \"What are the evaluation metrics and benchmarks used?\",\n    \"How will the results be analyzed and interpreted?\"\n  ],\n  \"Limitations\": [\n    \"Lack of concrete details and a clear direction\",\n    \"Preliminary results may not be robust or reliable\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 2,\n  \"Presentation\": 2,\n  \"Contribution\": 2,\n  \"Overall\": 6,\n  \"Confidence\": 3,\n  \"Decision\": \"Major Revision\"\n}\n\nReviewer #2:\n{\n  \"Summary\": \"The paper proposes to study pruning strategies for transformer-based language models, but the approach seems too straightforward and lacks depth.\",\n  \"Strengths\": [\n    \"The paper identifies a relevant problem in NLP\",\n    \"The proposed approach is well-defined and easy to understand\"\n  ],\n  \"Weaknesses\": [\n    \"The approach is not particularly innovative or impactful\",\n    \"The paper lacks depth and nuance in its analysis\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 3,\n  \"Clarity\": 4,\n  \"Significance\": 2,\n  \"Questions\": [\n    \"How does the proposed approach compare to existing pruning methods?\",\n    \"What are the trade-offs between different pruning strategies?\"\n  ],\n  \"Limitations\": [\n    \"The paper only explores a limited set of pruning strategies\",\n    \"The analysis is limited to a few benchmarks\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 2,\n  \"Overall\": 6,\n  \"Confidence\": 3,\n  \"Decision\": \"Major Revision\"\n}\n\nReviewer #3:\n{\n  \"Summary\": \"This paper proposes to study pruning strategies for transformer-based language models to reduce inference cost while preserving downstream task accuracy.\",\n  \"Strengths\": [\n    \"Timely and relevant topic\",\n    \"Clear presentation of methods and results\"\n  ],\n  \"Weaknesses\": [\n    \"Lack of novel contribution\",\n    \"Preliminary results may not be conclusive\"\n  ],\n  \"Originality\": 2,\n  \"Quality\": 3,\n  \"Clarity\": 4,\n  \"Significance\": 3,\n  \"Questions\": [\n    \"What are the key factors that affect the performance of pruning methods?\",\n    \"How do the proposed methods compare to state-of-the-art pruning techniques?\"\n  ],\n  \"Limitations\": [\n    \"Preliminary results may not be representative of the full scope of the problem\",\n    \"The evaluation of pruning methods may be too narrow\"\n  ],\n  \"Ethical Concerns\": false,\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 2,\n  \"Overall\": 7,\n  \"Confidence\": 4,\n  \"Decision\": \"Major Revision\"\n}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Minimal demo: run Exploration & Discovery (review loop)\n",
    "\n",
    "# Example inputs (replace with real plan / LaTeX report)\n",
    "plan = \"\"\"\n",
    "We propose to study pruning strategies for transformer-based language models.\n",
    "The goal is to reduce inference cost while preserving downstream task accuracy.\n",
    "Experiments will compare structured vs unstructured pruning across multiple layers.\n",
    "\"\"\"\n",
    "\n",
    "report = r\"\"\"\n",
    "\\section{Introduction}\n",
    "Transformer models are expensive to deploy. This paper explores pruning methods\n",
    "to reduce inference cost while maintaining performance.\n",
    "\n",
    "\\section{Method}\n",
    "We evaluate magnitude-based pruning and structured head pruning on several benchmarks.\n",
    "\n",
    "\\section{Results}\n",
    "Preliminary results show structured pruning preserves accuracy better at high sparsity.\n",
    "\"\"\"\n",
    "\n",
    "# Run reviewers\n",
    "output = reviewers_agent.inference(plan, report)\n",
    "\n",
    "print(\"=== Reviewer Feedback ===\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db058cd3-ba53-413f-ba2d-3dbe6f916c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregate Summary ===\n{\n  \"n_reviews\": 3,\n  \"overall_avg\": 6.0,\n  \"overall_min\": 6,\n  \"overall_max\": 6,\n  \"confidence_avg\": 3.0,\n  \"decision_majority\": \"Reject\",\n  \"decision_counts\": {\n    \"Accept\": 0,\n    \"Reject\": 0\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Aggregate reviewer JSON into a single decision + quick score summary\n",
    "\n",
    "import re\n",
    "\n",
    "def safe_get(d: dict, key: str, default=None):\n",
    "    return d.get(key, default) if isinstance(d, dict) else default\n",
    "\n",
    "def aggregate_reviews(reviews: list) -> dict:\n",
    "    \"\"\"\n",
    "    reviews: list of review_json dicts (can include None)\n",
    "    Returns a compact aggregate summary.\n",
    "    \"\"\"\n",
    "    valid = [r for r in reviews if isinstance(r, dict)]\n",
    "    if not valid:\n",
    "        return {\"error\": \"No valid review_json objects to aggregate.\"}\n",
    "\n",
    "    overalls = [safe_get(r, \"Overall\") for r in valid if isinstance(safe_get(r, \"Overall\"), (int, float))]\n",
    "    confidences = [safe_get(r, \"Confidence\") for r in valid if isinstance(safe_get(r, \"Confidence\"), (int, float))]\n",
    "    decisions = [safe_get(r, \"Decision\") for r in valid if isinstance(safe_get(r, \"Decision\"), str)]\n",
    "\n",
    "    # Majority decision\n",
    "    decision_norm = [d.strip().lower() for d in decisions]\n",
    "    accept_count = sum(d == \"accept\" for d in decision_norm)\n",
    "    reject_count = sum(d == \"reject\" for d in decision_norm)\n",
    "    majority = \"Accept\" if accept_count > reject_count else \"Reject\"\n",
    "\n",
    "    summary = {\n",
    "        \"n_reviews\": len(valid),\n",
    "        \"overall_avg\": sum(overalls) / len(overalls) if overalls else None,\n",
    "        \"overall_min\": min(overalls) if overalls else None,\n",
    "        \"overall_max\": max(overalls) if overalls else None,\n",
    "        \"confidence_avg\": sum(confidences) / len(confidences) if confidences else None,\n",
    "        \"decision_majority\": majority,\n",
    "        \"decision_counts\": {\"Accept\": accept_count, \"Reject\": reject_count},\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "r1 = get_score(plan, report, \"You are a harsh but fair reviewer and expect good experiments that lead to insights for the research topic.\", attempts=2)\n",
    "r2 = get_score(plan, report, \"You are a harsh and critical but fair reviewer who is looking for an idea that would be impactful in the field.\", attempts=2)\n",
    "r3 = get_score(plan, report, \"You are a harsh but fair open-minded reviewer that is looking for novel ideas that have not been proposed before.\", attempts=2)\n",
    "\n",
    "agg = aggregate_reviews([r1.get(\"review_json\"), r2.get(\"review_json\"), r3.get(\"review_json\")])\n",
    "\n",
    "print(\"=== Aggregate Summary ===\")\n",
    "print(json.dumps(agg, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ae9a85-6f93-4c94-9b24-d7ee9f59fbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7 — Revise the plan based on reviewer feedback (Databricks-native)\n",
    "\n",
    "def parse_json_strict(raw: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse JSON from model output that may be wrapped in ```json ... ``` fences.\n",
    "    Assumes the content inside the fence is valid JSON.\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        raise ValueError(\"Empty model output\")\n",
    "\n",
    "    text = raw.strip()\n",
    "\n",
    "    # Strip ```json / ``` fences if present\n",
    "    if text.startswith(\"```\"):\n",
    "        lines = text.splitlines()\n",
    "\n",
    "        # remove opening fence (``` or ```json)\n",
    "        if lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "\n",
    "        # remove closing fence\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "\n",
    "        text = \"\\n\".join(lines).strip()\n",
    "\n",
    "    return json.loads(text)\n",
    "\n",
    "def revise_plan_with_reviews(original_plan: str, review_jsons: list) -> dict:\n",
    "    valid_reviews = [r for r in review_jsons if isinstance(r, dict)]\n",
    "\n",
    "    sys_prompt = \"\"\"\n",
    "You are a research lead improving an experiment plan based on peer review feedback.\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no extra text) with keys:\n",
    "- revised_plan (string)\n",
    "- change_log (array of strings)\n",
    "\"\"\".strip()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ORIGINAL PLAN:\n",
    "{original_plan}\n",
    "\n",
    "REVIEWER FEEDBACK (JSON):\n",
    "{json.dumps(valid_reviews, indent=2)}\n",
    "\n",
    "Return ONLY JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "    raw = query_model(sys_prompt, prompt)\n",
    "\n",
    "    try:\n",
    "        return parse_json_strict(raw)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Failed to parse JSON: {e}\",\n",
    "            \"raw\": raw,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b106c7d-f0ba-4933-98d8-a7343f7dd79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Revised Plan Output ===\n{\n  \"revised_plan\": \"We propose to study pruning strategies for transformer-based language models. The goal is to reduce inference cost while preserving downstream task accuracy. Experiments will compare structured vs unstructured pruning across multiple layers, with a focus on implementing and evaluating pruning strategies using specific quantitative metrics, such as accuracy and inference cost. We will also investigate the implications of high sparsity on downstream task accuracy and explore the trade-off between inference cost and accuracy.\",\n  \"change_log\": [\n    \"Added specific quantitative metrics to evaluate pruning strategies\",\n    \"Investigated implications of high sparsity on downstream task accuracy\",\n    \"Explored trade-off between inference cost and accuracy\",\n    \"Provided more depth and detail in the paper\",\n    \"Presented preliminary results in a more thorough and analyzed manner\"\n  ]\n}\n"
     ]
    }
   ],
   "source": [
    "revised = revise_plan_with_reviews(\n",
    "    original_plan=plan,\n",
    "    review_jsons=[r1.get(\"review_json\"), r2.get(\"review_json\"), r3.get(\"review_json\")]\n",
    ")\n",
    "\n",
    "print(\"=== Revised Plan Output ===\")\n",
    "print(json.dumps(revised, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2fbd3f-8ee8-409d-9423-ad5de0206a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================= ROUND 1 =================\nAggregate: {\n  \"n_reviews\": 3,\n  \"overall_avg\": 6.0,\n  \"overall_min\": 6,\n  \"overall_max\": 6,\n  \"confidence_avg\": 3.0,\n  \"decision_majority\": \"Reject\",\n  \"decision_counts\": {\n    \"Accept\": 0,\n    \"Reject\": 0\n  }\n}\n\n--- Revised plan (first 600 chars) ---\nWe propose to study pruning strategies for transformer-based language models, focusing on a novel comparison of structured vs unstructured pruning across multiple layers. Experiments will be designed to ensure robust results, with a clear research question and expected outcomes. The pruning strategies will be evaluated on multiple tasks, and the computational requirements will be analyzed.\n\n================= ROUND 2 =================\nAggregate: {\n  \"n_reviews\": 3,\n  \"overall_avg\": 6.0,\n  \"overall_min\": 6,\n  \"overall_max\": 6,\n  \"confidence_avg\": 3.0,\n  \"decision_majority\": \"Reject\",\n  \"decision_counts\": {\n    \"Accept\": 0,\n    \"Reject\": 1\n  }\n}\n\n--- Revised plan (first 600 chars) ---\nWe propose to study pruning strategies for transformer-based language models, focusing on a novel comparison of structured vs unstructured pruning across multiple layers. Experiments will be designed to ensure robust results, with a clear research question and expected outcomes. The pruning strategies will be implemented and evaluated using a combination of metrics, including accuracy, computational requirements, and model size. The experiments will be conducted on multiple tasks, including language translation, sentiment analysis, and text classification. The computational requirements will b\n\n================= ROUND 3 =================\nAggregate: {\n  \"n_reviews\": 3,\n  \"overall_avg\": 6.0,\n  \"overall_min\": 6,\n  \"overall_max\": 6,\n  \"confidence_avg\": 3.0,\n  \"decision_majority\": \"Reject\",\n  \"decision_counts\": {\n    \"Accept\": 0,\n    \"Reject\": 0\n  }\n}\n\n--- Revised plan (first 600 chars) ---\nWe propose to study pruning strategies for transformer-based language models, focusing on a novel comparison of structured vs unstructured pruning across multiple layers. Experiments will be designed to ensure robust results, with a clear research question and expected outcomes. The pruning strategies will be implemented and evaluated using a combination of metrics, including accuracy, computational requirements, and model size. The experiments will be conducted on multiple tasks, including language translation, sentiment analysis, and text classification. The computational requirements will b\n\n=== FINAL PLAN (first 800 chars) ===\nWe propose to study pruning strategies for transformer-based language models, focusing on a novel comparison of structured vs unstructured pruning across multiple layers. Experiments will be designed to ensure robust results, with a clear research question and expected outcomes. The pruning strategies will be implemented and evaluated using a combination of metrics, including accuracy, computational requirements, and model size. The experiments will be conducted on multiple tasks, including language translation, sentiment analysis, and text classification. The computational requirements will be analyzed using tools such as PyTorch's TensorBoard and NVIDIA's Nsight Systems. The results will be presented in a clear and concise manner, with a focus on the strengths and weaknesses of each prun\n"
     ]
    }
   ],
   "source": [
    "# Cell C — Iteration loop: review -> aggregate -> revise (stop on Accept or threshold)\n",
    "\n",
    "def run_iteration_loop(\n",
    "    initial_plan: str,\n",
    "    initial_report: str,\n",
    "    max_rounds: int = 3,\n",
    "    overall_threshold: float = 7.5,\n",
    "):\n",
    "    plan_cur = initial_plan\n",
    "    report_cur = initial_report  # keep constant here; you can revise report similarly later\n",
    "\n",
    "    history = []\n",
    "\n",
    "    reviewer_1 = \"You are a harsh but fair reviewer and expect good experiments that lead to insights for the research topic.\"\n",
    "    reviewer_2 = \"You are a harsh and critical but fair reviewer who is looking for an idea that would be impactful in the field.\"\n",
    "    reviewer_3 = \"You are a harsh but fair open-minded reviewer that is looking for novel ideas that have not been proposed before.\"\n",
    "\n",
    "    for rnd in range(1, max_rounds + 1):\n",
    "        print(f\"\\n================= ROUND {rnd} =================\")\n",
    "\n",
    "        r1 = get_score(plan_cur, report_cur, reviewer_1, attempts=2)\n",
    "        r2 = get_score(plan_cur, report_cur, reviewer_2, attempts=2)\n",
    "        r3 = get_score(plan_cur, report_cur, reviewer_3, attempts=2)\n",
    "\n",
    "        reviews = [r1.get(\"review_json\"), r2.get(\"review_json\"), r3.get(\"review_json\")]\n",
    "        agg = aggregate_reviews(reviews)\n",
    "\n",
    "        print(\"Aggregate:\", json.dumps(agg, indent=2))\n",
    "\n",
    "        history.append({\n",
    "            \"round\": rnd,\n",
    "            \"plan\": plan_cur,\n",
    "            \"agg\": agg,\n",
    "            \"reviews\": reviews,\n",
    "        })\n",
    "\n",
    "        # Stop conditions\n",
    "        if agg.get(\"decision_majority\") == \"Accept\":\n",
    "            print(\"✅ Stop: majority decision is Accept\")\n",
    "            break\n",
    "\n",
    "        overall_avg = agg.get(\"overall_avg\")\n",
    "        if isinstance(overall_avg, (int, float)) and overall_avg >= overall_threshold:\n",
    "            print(f\"✅ Stop: overall_avg >= threshold ({overall_avg:.2f} >= {overall_threshold})\")\n",
    "            break\n",
    "\n",
    "        # Revise plan based on reviews\n",
    "        revised = revise_plan_with_reviews(plan_cur, reviews)\n",
    "        if \"error\" in revised:\n",
    "            print(\"⚠️ Revise failed, stopping. Error:\", revised[\"error\"])\n",
    "            break\n",
    "\n",
    "        plan_cur = revised[\"revised_plan\"]\n",
    "        print(\"\\n--- Revised plan (first 600 chars) ---\")\n",
    "        print(plan_cur[:600])\n",
    "\n",
    "    return {\"final_plan\": plan_cur, \"history\": history}\n",
    "\n",
    "\n",
    "result = run_iteration_loop(\n",
    "    initial_plan=plan,\n",
    "    initial_report=report,\n",
    "    max_rounds=3,\n",
    "    overall_threshold=7.5,\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL PLAN (first 800 chars) ===\")\n",
    "print(result[\"final_plan\"][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76709e25-7f3d-4838-b6da-11951941f54c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14_Lab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}