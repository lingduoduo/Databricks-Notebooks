{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.7\n  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\nCollecting langgraph==0.5.3\n  Downloading langgraph-0.5.3-py3-none-any.whl.metadata (6.9 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (6.0)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.7)\n  Downloading sqlalchemy-2.0.45-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.3.7)\n  Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\nCollecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n  Downloading langchain_core-0.3.83-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.7)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (1.23.5)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain==0.3.7)\n  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.3.7) (8.2.2)\nCollecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph==0.5.3)\n  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\nCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph==0.5.3)\n  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.5.3)\n  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\nCollecting xxhash>=3.5.0 (from langgraph==0.5.3)\n  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain_community\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.7)\n  Downloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting databricks-vectorsearch<0.41,>=0.40 (from langchain-databricks)\n  Downloading databricks_vectorsearch-0.40-py3-none-any.whl.metadata (2.8 kB)\nCollecting mlflow>=2.9 (from langchain-databricks)\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting numpy<2,>=1 (from langchain==0.3.7)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from langchain-databricks) (1.11.1)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading frozenlist-1.8.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.7)\n  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.11.4)\nCollecting protobuf<5,>=3.12.0 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\nCollecting deprecation>=2 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n  Downloading langchain_core-0.3.82-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.80-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.77-py3-none-any.whl.metadata (3.2 kB)\n  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\nINFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7) (4.10.0)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\nCollecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading orjson-3.11.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting pip>=25.2 (from langchain-text-splitters<0.4.0,>=0.3.0->langchain==0.3.7)\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.5.3)\n  Downloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.2 kB)\nCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph==0.5.3)\n  Downloading langgraph_prebuilt-0.5.1-py3-none-any.whl.metadata (4.5 kB)\nINFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\nCollecting mlflow>=2.9 (from langchain-databricks)\n  Downloading mlflow-3.8.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.5.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\nINFO: pip is still looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.3-py3-none-any.whl.metadata (29 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\nCollecting Flask<4 (from mlflow>=2.9->langchain-databricks)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting Jinja2<4,>=2.11 (from mlflow>=2.9->langchain-databricks)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow>=2.9->langchain-databricks)\n  Downloading alembic-1.18.1-py3-none-any.whl.metadata (7.2 kB)\nCollecting docker<8,>=4.0.0 (from mlflow>=2.9->langchain-databricks)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow>=2.9->langchain-databricks)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow>=2.9->langchain-databricks)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting markdown<4,>=3.3 (from mlflow>=2.9->langchain-databricks)\n  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (3.7.2)\nRequirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (1.5.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (14.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.9->langchain-databricks) (1.3.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.0.4)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.40.0)\nCollecting fastapi<1 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.0.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.1)\nCollecting uvicorn<1 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\nCollecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.3 kB)\nCollecting typing-extensions>=4.7 (from langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.7)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.7) (2023.7.22)\nCollecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.3.7)\n  Downloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.9->langchain-databricks)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting click<9,>=7.0 (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting markupsafe>=2.1.1 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (2.7 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.9->langchain-databricks)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.9->langchain-databricks)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.9->langchain-databricks)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow>=2.9->langchain-databricks) (2.8.2)\nCollecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.7)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain==0.3.7)\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.9->langchain-databricks) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas!=2.3.0,<3->mlflow>=2.9->langchain-databricks) (2022.7)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.9->langchain-databricks) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.9->langchain-databricks) (2.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.35.0)\nCollecting starlette<0.51.0,>=0.40.0 (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\nCollecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.11.0)\nCollecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.9->langchain-databricks) (1.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\nDownloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langgraph-0.5.3-py3-none-any.whl (143 kB)\nDownloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m78.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_databricks-0.1.2-py3-none-any.whl (21 kB)\nDownloading aiohttp-3.13.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m41.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.40-py3-none-any.whl (12 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\nDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\nDownloading langgraph_prebuilt-0.5.1-py3-none-any.whl (23 kB)\nDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\nDownloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/29.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.0/29.0 MB\u001B[0m \u001B[31m151.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m121.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/14.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.2/14.2 MB\u001B[0m \u001B[31m106.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\nDownloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m69.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m73.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (213 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading alembic-1.18.1-py3-none-any.whl (260 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading frozenlist-1.8.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (233 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (577 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/577.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m577.1/577.1 kB\u001B[0m \u001B[31m26.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading markdown-3.10-py3-none-any.whl (107 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading multidict-6.7.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (246 kB)\nDownloading orjson-3.11.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\nDownloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (202 kB)\nDownloading propcache-0.4.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (214 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl (294 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading yarl-1.22.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (368 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading click-8.3.1-py3-none-any.whl (108 kB)\nDownloading fastapi-0.128.0-py3-none-any.whl (103 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (24 kB)\nDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\nDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\nDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\nDownloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading starlette-0.50.0-py3-none-any.whl (74 kB)\nInstalling collected packages: xxhash, typing-extensions, python-dotenv, protobuf, propcache, ormsgpack, orjson, numpy, multidict, marshmallow, markupsafe, markdown, jsonpointer, itsdangerous, httpx-sse, h11, gunicorn, greenlet, graphql-core, frozenlist, deprecation, click, blinker, attrs, annotated-types, annotated-doc, aiohappyeyeballs, yarl, werkzeug, uvicorn, typing-inspection, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic-core, opentelemetry-api, Mako, jsonpatch, Jinja2, httpcore, graphql-relay, docker, anyio, aiosignal, starlette, pydantic, opentelemetry-semantic-conventions, httpx, graphene, Flask, dataclasses-json, alembic, aiohttp, pydantic-settings, opentelemetry-sdk, langsmith, langgraph-sdk, fastapi, mlflow-skinny, langchain-core, mlflow, langgraph-checkpoint, langchain-text-splitters, databricks-vectorsearch, langgraph-prebuilt, langchain-databricks, langchain, langgraph, langchain_community\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.3\n    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'click'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.4\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.11.4\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-643f9891-312b-4283-bbad-da939bb0e7df\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.69.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed Flask-3.1.2 Jinja2-3.1.6 Mako-1.3.10 SQLAlchemy-2.0.35 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 alembic-1.18.1 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.12.1 attrs-25.4.0 blinker-1.9.0 click-8.3.1 databricks-vectorsearch-0.40 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 fastapi-0.128.0 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 itsdangerous-2.2.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.7 langchain-core-0.3.63 langchain-databricks-0.1.2 langchain-text-splitters-0.3.8 langchain_community-0.3.7 langgraph-0.5.3 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.5.1 langgraph-sdk-0.1.74 langsmith-0.1.147 markdown-3.10 markupsafe-3.0.3 marshmallow-3.26.2 mlflow-2.22.4 mlflow-skinny-2.22.4 multidict-6.7.0 numpy-1.26.4 opentelemetry-api-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 orjson-3.11.5 ormsgpack-1.12.1 propcache-0.4.1 protobuf-4.25.8 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 requests-toolbelt-1.0.0 starlette-0.50.0 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 uvicorn-0.40.0 werkzeug-3.1.5 xxhash-3.6.0 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "%pip install -U langchain==0.3.7 langgraph==0.5.3 langchain_community langchain-databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c57fc7-083e-4750-87ef-2391de68d4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain.schema import HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229659aa-466d-4ad1-b616-c754c0586406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-643f9891-312b-4283-bbad-da/.ipykernel/3661109/command-1575010149449492-2723535277:3: LangChainDeprecationWarning: The class `ChatDatabricks` was deprecated in LangChain 0.3.3 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-databricks package and should be used instead. To use it run `pip install -U :class:`~langchain-databricks` and import as `from :class:`~langchain_databricks import ChatDatabricks``.\n  llm = ChatDatabricks(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\" \n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_ENDPOINT_NAME,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6e5561-875a-488f-a0de-08bb989e6823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedd8034-3aef-47d4-ba75-29d09af2f410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response accuracy (exact match baseline): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Exact-match \"accuracy\" baseline\n",
    "\n",
    "def evaluate_response_accuracy(agent_output: str, expected_output: str) -> float:\n",
    "    \"\"\"Calculates a simple exact-match accuracy score (baseline).\"\"\"\n",
    "    return 1.0 if agent_output.strip().lower() == expected_output.strip().lower() else 0.0\n",
    "\n",
    "# Quick sanity test\n",
    "agent_response = \"The capital of France is Paris.\"\n",
    "ground_truth = \"Paris is the capital of France.\"\n",
    "score = evaluate_response_accuracy(agent_response, ground_truth)\n",
    "\n",
    "print(f\"Response accuracy (exact match baseline): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31eda9d9-0e39-4488-b153-320a55e66025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 'simulated_tool_call' took 150.08 ms\nTool call result: Result for 'get weather'\n"
     ]
    }
   ],
   "source": [
    "def timed_agent_action(agent_function: Callable, *args, **kwargs) -> Tuple[any, float]:\n",
    "    \"\"\"Measures execution time of an agent function in milliseconds.\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    result = agent_function(*args, **kwargs)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    print(f\"Action '{agent_function.__name__}' took {latency_ms:.2f} ms\")\n",
    "    \n",
    "    return result, latency_ms\n",
    "\n",
    "# Dummy agent/tool simulation\n",
    "def simulated_tool_call(query: str) -> str:\n",
    "    time.sleep(0.15)  # simulate IO / model latency\n",
    "    return f\"Result for '{query}'\"\n",
    "\n",
    "result, latency = timed_agent_action(simulated_tool_call, \"get weather\")\n",
    "print(f\"Tool call result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98f81f1-3d41-4303-9afb-a9755623796f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded interaction: input_tokens≈6, output_tokens≈6\nRecorded interaction: input_tokens≈4, output_tokens≈10\nTotal input tokens≈10, Total output tokens≈16\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Simple interaction monitor (placeholder token counting)\n",
    "\n",
    "class LLMInteractionMonitor:\n",
    "    \"\"\"\n",
    "    Tracks approximate input/output tokens.\n",
    "    NOTE: This uses word-count as a placeholder; real token counts should come\n",
    "    from the LLM API usage fields or a tokenizer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "\n",
    "    def record_interaction(self, prompt: str, response: str):\n",
    "        input_tokens = len(prompt.split())     # placeholder\n",
    "        output_tokens = len(response.split())  # placeholder\n",
    "\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "\n",
    "        print(\n",
    "            f\"Recorded interaction: \"\n",
    "            f\"input_tokens≈{input_tokens}, output_tokens≈{output_tokens}\"\n",
    "        )\n",
    "\n",
    "    def get_total_tokens(self):\n",
    "        return self.total_input_tokens, self.total_output_tokens\n",
    "\n",
    "\n",
    "# Example usage\n",
    "monitor = LLMInteractionMonitor()\n",
    "monitor.record_interaction(\"What is the capital of France?\", \"The capital of France is Paris.\")\n",
    "monitor.record_interaction(\n",
    "    \"Tell me a joke.\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    ")\n",
    "\n",
    "input_t, output_t = monitor.get_total_tokens()\n",
    "print(f\"Total input tokens≈{input_t}, Total output tokens≈{output_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ea3ddd-f9a6-4b9a-b9ab-166eaa377888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard: 1.0\nBinary: 1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # keep words/numbers, remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def jaccard_similarity(a: str, b: str) -> float:\n",
    "    a_tokens = set(normalize_text(a).split())\n",
    "    b_tokens = set(normalize_text(b).split())\n",
    "    if not a_tokens and not b_tokens:\n",
    "        return 1.0\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    return len(a_tokens & b_tokens) / len(a_tokens | b_tokens)\n",
    "\n",
    "def evaluate_response_accuracy_jaccard(agent_output: str, expected_output: str, threshold: float = 0.6) -> float:\n",
    "    \"\"\"\n",
    "    Returns 1.0 if token-set overlap is high enough; else 0.0.\n",
    "    Threshold 0.6 works well for short factual sentences like this.\n",
    "    \"\"\"\n",
    "    score = jaccard_similarity(agent_output, expected_output)\n",
    "    return 1.0 if score >= threshold else 0.0\n",
    "\n",
    "\n",
    "# quick check\n",
    "print(\"Jaccard:\", jaccard_similarity(\"The capital of France is Paris.\", \"Paris is the capital of France.\"))\n",
    "print(\"Binary:\", evaluate_response_accuracy_jaccard(\"The capital of France is Paris.\", \"Paris is the capital of France.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcc7f4b-e9a3-40ab-bdf1-a58e8861f7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Databricks LLM configured: databricks-meta-llama-3-1-8b-instruct\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-643f9891-312b-4283-bbad-da/.ipykernel/3661109/command-1575010149449506-2511559838:12: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=LLM_ENDPOINT_NAME,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(\"✅ Databricks LLM configured:\", LLM_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780038b0-337e-4edd-ba96-41c2248d6a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rubric loaded (strict JSON format enforced)\n"
     ]
    }
   ],
   "source": [
    "LEGAL_SURVEY_RUBRIC = \"\"\"\n",
    "You are an expert legal survey methodologist and a critical legal reviewer. Your task is to evaluate the quality of a given legal survey question.\n",
    "\n",
    "Provide a score from 1 to 5 for overall quality, along with a detailed rationale and specific feedback.\n",
    "Focus on the following criteria:\n",
    "\n",
    "1. **Clarity & Precision (Score 1-5):**\n",
    "   - 1: Extremely vague, highly ambiguous, or confusing.\n",
    "   - 3: Moderately clear, but could be more precise.\n",
    "   - 5: Perfectly clear, unambiguous, and precise in its legal terminology (if applicable) and intent.\n",
    "\n",
    "2. **Neutrality & Bias (Score 1-5):**\n",
    "   - 1: Highly leading or biased, clearly influencing the respondent towards a specific answer.\n",
    "   - 3: Slightly suggestive or could be interpreted as leading.\n",
    "   - 5: Completely neutral, objective, and free from any leading language or loaded terms.\n",
    "\n",
    "3. **Relevance & Focus (Score 1-5):**\n",
    "   - 1: Irrelevant to the stated survey topic or out of scope.\n",
    "   - 3: Loosely related but could be more focused.\n",
    "   - 5: Directly relevant to the survey's objectives and well-focused on a single concept.\n",
    "\n",
    "4. **Completeness (Score 1-5):**\n",
    "   - 1: Omits critical information needed to answer accurately or provides insufficient context.\n",
    "   - 3: Mostly complete, but minor details are missing.\n",
    "   - 5: Provides all necessary context and information for the respondent to answer thoroughly.\n",
    "\n",
    "5. **Appropriateness for Audience (Score 1-5):**\n",
    "   - 1: Uses jargon inaccessible to the target audience or is overly simplistic for experts.\n",
    "   - 3: Generally appropriate, but some terms might be challenging or oversimplified.\n",
    "   - 5: Perfectly tailored to the assumed legal knowledge and background of the target survey audience.\n",
    "\n",
    "**Output Format (STRICT):**\n",
    "Return ONLY a valid JSON object with EXACTLY these keys:\n",
    "- overall_score (integer 1-5)\n",
    "- rationale (string)\n",
    "- detailed_feedback (array of strings; each bullet covers one criterion)\n",
    "- concerns (array of strings)\n",
    "- recommended_action (string)\n",
    "\n",
    "No markdown. No extra text.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"✅ Rubric loaded (strict JSON format enforced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ff604b-4510-44ac-a006-b69ec249a8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Judge initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — LLM Judge class (Databricks-native, strict JSON handling)\n",
    "\n",
    "class LLMJudgeForLegalSurvey:\n",
    "    \"\"\"\n",
    "    Evaluates legal survey questions using a Databricks-hosted chat LLM.\n",
    "    Enforces strict JSON output and includes basic recovery for minor formatting issues.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: ChatDatabricks):\n",
    "        self.llm = llm\n",
    "\n",
    "    def _build_messages(self, survey_question: str):\n",
    "        return [\n",
    "            SystemMessage(content=LEGAL_SURVEY_RUBRIC),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    \"LEGAL SURVEY QUESTION TO EVALUATE:\\n\"\n",
    "                    f\"{survey_question}\\n\\n\"\n",
    "                    \"Remember: return ONLY valid JSON with the specified keys.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def judge(self, survey_question: str) -> Optional[Dict[str, Any]]:\n",
    "        messages = self._build_messages(survey_question)\n",
    "\n",
    "        try:\n",
    "            logging.info(\"Sending survey question to Databricks LLM for evaluation...\")\n",
    "            response = self.llm.invoke(messages)\n",
    "            raw_text = (response.content or \"\").strip()\n",
    "\n",
    "            # First attempt: direct JSON parse\n",
    "            try:\n",
    "                return json.loads(raw_text)\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback: extract first JSON object if model added extra text\n",
    "                start = raw_text.find(\"{\")\n",
    "                end = raw_text.rfind(\"}\")\n",
    "                if start != -1 and end != -1 and end > start:\n",
    "                    candidate = raw_text[start : end + 1]\n",
    "                    return json.loads(candidate)\n",
    "\n",
    "                logging.error(\"JSON parsing failed. Raw response:\\n%s\", raw_text)\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(\"Unexpected error during LLM judgment: %s\", e)\n",
    "            return None\n",
    "\n",
    "\n",
    "# Instantiate the judge\n",
    "judge = LLMJudgeForLegalSurvey(llm)\n",
    "print(\"✅ LLM Judge initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c716799-5891-4e8e-accc-9482e5c82cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Example survey questions prepared\n"
     ]
    }
   ],
   "source": [
    "good_legal_survey_question = \"\"\"\n",
    "To what extent do you agree or disagree that current intellectual property laws in Switzerland\n",
    "adequately protect emerging AI-generated content, assuming the content meets the originality\n",
    "criteria established by the Federal Supreme Court?\n",
    "(Select one: Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\n",
    "\"\"\".strip()\n",
    "\n",
    "biased_legal_survey_question = \"\"\"\n",
    "Don't you agree that overly restrictive data privacy laws like the FADP are hindering essential\n",
    "technological innovation and economic growth in Switzerland?\n",
    "(Select one: Yes, No)\n",
    "\"\"\".strip()\n",
    "\n",
    "vague_legal_survey_question = \"\"\"\n",
    "What are your thoughts on legal tech?\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"✅ Example survey questions prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be9c108-d5f6-427a-b6bc-8d637e07a493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:40:08,135 - INFO - Sending survey question to Databricks LLM for evaluation...\n2026-01-16 20:40:09,892 - INFO - Sending survey question to Databricks LLM for evaluation...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nEvaluating GOOD legal survey question\n================================================================================\n{\n  \"overall_score\": 4,\n  \"rationale\": \"The question is well-structured and clear, but it could benefit from a slight adjustment to ensure neutrality.\",\n  \"detailed_feedback\": [\n    \"Clarity & Precision: 5 - The question is perfectly clear and precise in its legal terminology and intent.\",\n    \"Neutrality & Bias: 4 - The question is mostly neutral, but the assumption that the content meets the originality criteria might slightly influence the respondent's answer.\",\n    \"Relevance & Focus: 5 - The question is directly relevant to the survey's objectives and well-focused on a single concept.\",\n    \"Completeness: 5 - The question provides all necessary context and information for the respondent to answer thoroughly.\",\n    \"Appropriateness for Audience: 5 - The question is perfectly tailored to the assumed legal knowledge and background of the target survey audience.\"\n  ],\n  \"concerns\": [\n    \"The assumption that the content meets the originality criteria might influence the respondent's answer, potentially leading to biased results.\",\n    \"The question does not explicitly define what is meant by 'emerging AI-generated content', which might cause confusion among respondents.\"\n  ],\n  \"recommended_action\": \"Consider revising the question to remove the assumption about the content meeting the originality criteria, and consider adding a definition of 'emerging AI-generated content' to ensure clarity and neutrality.\"\n}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:40:11,819 - INFO - Sending survey question to Databricks LLM for evaluation...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nEvaluating BIASED legal survey question\n================================================================================\n{\n  \"overall_score\": 2,\n  \"rationale\": \"The question has some issues with clarity, neutrality, and relevance.\",\n  \"detailed_feedback\": [\n    \"Clarity & Precision: The question is somewhat ambiguous due to the use of 'overly restrictive' and 'hindering essential technological innovation and economic growth', which could be interpreted in different ways.\",\n    \"Neutrality & Bias: The question is leading and biased towards a negative view of the FADP, using words like 'hindering' and 'overly restrictive', which may influence respondents to choose 'Yes'.\",\n    \"Relevance & Focus: The question is somewhat off-topic, as it focuses on the impact of the FADP on innovation and economic growth, rather than the data privacy laws themselves.\",\n    \"Completeness: The question lacks context and information about what is meant by 'essential technological innovation and economic growth', making it difficult for respondents to answer accurately.\",\n    \"Appropriateness for Audience: The question assumes a certain level of knowledge about the FADP and its implications, which may not be shared by all respondents.\"\n  ],\n  \"concerns\": [\n    \"The question may elicit biased responses due to its leading language.\",\n    \"The question may not accurately capture respondents' views on data privacy laws.\",\n    \"The question may be difficult for respondents to answer accurately due to its ambiguity and lack of context.\"\n  ],\n  \"recommended_action\": \"Rephrase the question to make it more neutral, clear, and focused on the data privacy laws themselves, rather than their impact on innovation and economic growth.\"\n}\n\n================================================================================\nEvaluating VAGUE legal survey question\n================================================================================\n{\n  \"overall_score\": 2,\n  \"rationale\": \"The question is too vague and open-ended, leading to a wide range of possible answers that may not be comparable or useful for analysis.\",\n  \"detailed_feedback\": [\n    \"Clarity & Precision: The question is extremely vague and lacks precision in its legal terminology.\",\n    \"Neutrality & Bias: The question is neutral but could be more specific to elicit meaningful responses.\",\n    \"Relevance & Focus: The question is somewhat relevant to legal issues but lacks focus on a specific concept or area.\",\n    \"Completeness: The question omits critical information needed to answer accurately or provides insufficient context.\",\n    \"Appropriateness for Audience: The question is generally appropriate for a legal audience but may be too broad for experts.\"\n  ],\n  \"concerns\": [\n    \"The question may elicit non-technical or non-legal responses that are not relevant to the survey's objectives.\",\n    \"The question may not capture the nuances of legal tech or its applications in the legal field.\"\n  ],\n  \"recommended_action\": \"Revise the question to be more specific, focused, and relevant to the survey's objectives, such as 'What are your thoughts on the adoption of artificial intelligence in the legal profession?'\"\n}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Run evaluations + pretty-print JSON\n",
    "\n",
    "def pretty_print(label: str, result: Optional[Dict[str, Any]]):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(label)\n",
    "    print(\"=\" * 80)\n",
    "    if result is None:\n",
    "        print(\"❌ No judgment returned (JSON parse failed or request error).\")\n",
    "    else:\n",
    "        print(json.dumps(result, indent=2))\n",
    "\n",
    "pretty_print(\"Evaluating GOOD legal survey question\", judge.judge(good_legal_survey_question))\n",
    "pretty_print(\"Evaluating BIASED legal survey question\", judge.judge(biased_legal_survey_question))\n",
    "pretty_print(\"Evaluating VAGUE legal survey question\", judge.judge(vague_legal_survey_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdb50ec-123b-430e-9304-6a57d508649b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:40:35,704 - INFO - Sending survey question to Databricks LLM for evaluation...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output is schema-valid\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Strict schema validation for judge output\n",
    "\n",
    "from typing import List\n",
    "\n",
    "REQUIRED_SCHEMA = {\n",
    "    \"overall_score\": int,\n",
    "    \"rationale\": str,\n",
    "    \"detailed_feedback\": list,\n",
    "    \"concerns\": list,\n",
    "    \"recommended_action\": str,\n",
    "}\n",
    "\n",
    "def validate_judge_output(result: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Validates the LLM judge output against the required schema.\n",
    "    Returns a list of validation errors (empty if valid).\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for key, expected_type in REQUIRED_SCHEMA.items():\n",
    "        if key not in result:\n",
    "            errors.append(f\"Missing required key: '{key}'\")\n",
    "        else:\n",
    "            if not isinstance(result[key], expected_type):\n",
    "                errors.append(\n",
    "                    f\"Key '{key}' has wrong type: \"\n",
    "                    f\"expected {expected_type.__name__}, got {type(result[key]).__name__}\"\n",
    "                )\n",
    "\n",
    "    # Additional semantic checks\n",
    "    if \"overall_score\" in result:\n",
    "        score = result[\"overall_score\"]\n",
    "        if not isinstance(score, int) or not (1 <= score <= 5):\n",
    "            errors.append(\"overall_score must be an integer between 1 and 5\")\n",
    "\n",
    "    if \"detailed_feedback\" in result:\n",
    "        if not all(isinstance(x, str) for x in result[\"detailed_feedback\"]):\n",
    "            errors.append(\"detailed_feedback must be a list of strings\")\n",
    "\n",
    "    if \"concerns\" in result:\n",
    "        if not all(isinstance(x, str) for x in result[\"concerns\"]):\n",
    "            errors.append(\"concerns must be a list of strings\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# Example validation run\n",
    "sample_result = judge.judge(good_legal_survey_question)\n",
    "validation_errors = validate_judge_output(sample_result) if sample_result else [\"No result\"]\n",
    "\n",
    "print(\"Validation errors:\" if validation_errors else \"✅ Output is schema-valid\")\n",
    "for err in validation_errors:\n",
    "    print(\"-\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd74ed7-e382-4c4c-b056-2f18a92df8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Judge OK (attempt 1). Latency: 1771.06 ms\n\n================================================================================\nGOOD question (judge_with_repair)\n================================================================================\n{\n  \"overall_score\": 4,\n  \"rationale\": \"The question is well-structured and clear, but it could benefit from a slight adjustment to ensure neutrality.\",\n  \"detailed_feedback\": [\n    \"Clarity & Precision: 5 - The question is perfectly clear and precise in its legal terminology and intent.\",\n    \"Neutrality & Bias: 4 - The question is mostly neutral, but the phrase 'adequately protect' could be interpreted as slightly leading.\",\n    \"Relevance & Focus: 5 - The question is directly relevant to the survey's objectives and well-focused on a single concept.\",\n    \"Completeness: 5 - The question provides all necessary context and information for the respondent to answer thoroughly.\",\n    \"Appropriateness for Audience: 5 - The question is perfectly tailored to the assumed legal knowledge and background of the target survey audience.\"\n  ],\n  \"concerns\": [\n    \"The use of 'adequately protect' might influence respondents' answers, as it implies a level of protection that could be subjective.\",\n    \"The question assumes a basic understanding of the Federal Supreme Court's originality criteria, which might not be universally known.\"\n  ],\n  \"recommended_action\": \"Consider rephrasing the question to use more neutral language, such as 'To what extent do you agree or disagree that current intellectual property laws in Switzerland provide sufficient protection for emerging AI-generated content?'\"\n}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Auto-repair + retry when JSON is invalid (Databricks-native)\n",
    "\n",
    "import time\n",
    "\n",
    "def timed_invoke(messages):\n",
    "    start = time.perf_counter()\n",
    "    resp = llm.invoke(messages)\n",
    "    end = time.perf_counter()\n",
    "    return resp, (end - start) * 1000\n",
    "\n",
    "\n",
    "def _extract_json_object(text: str) -> Optional[Dict[str, Any]]:\n",
    "    text = (text or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(text[start : end + 1])\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def judge_with_repair(\n",
    "    survey_question: str,\n",
    "    max_attempts: int = 2,\n",
    "    print_debug: bool = True,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1) Try normal judge call\n",
    "    2) If JSON invalid or schema invalid, ask model to ONLY output corrected JSON (repair pass)\n",
    "    \"\"\"\n",
    "    # Attempt 1: normal judge\n",
    "    messages = judge._build_messages(survey_question)  # reuse existing builder\n",
    "    resp, latency_ms = timed_invoke(messages)\n",
    "    raw = (resp.content or \"\").strip()\n",
    "    parsed = _extract_json_object(raw)\n",
    "\n",
    "    if parsed is not None:\n",
    "        errs = validate_judge_output(parsed)\n",
    "        if not errs:\n",
    "            if print_debug:\n",
    "                print(f\"✅ Judge OK (attempt 1). Latency: {latency_ms:.2f} ms\")\n",
    "            return parsed\n",
    "        if print_debug:\n",
    "            print(f\"⚠️ Schema invalid (attempt 1). Latency: {latency_ms:.2f} ms\")\n",
    "            for e in errs:\n",
    "                print(\"-\", e)\n",
    "    else:\n",
    "        if print_debug:\n",
    "            print(f\"⚠️ JSON parse failed (attempt 1). Latency: {latency_ms:.2f} ms\")\n",
    "            print(\"Raw response:\\n\", raw[:1200])\n",
    "\n",
    "    # Repair attempts\n",
    "    attempt = 2\n",
    "    while attempt <= max_attempts:\n",
    "        repair_instructions = f\"\"\"\n",
    "You previously attempted to evaluate a legal survey question, but your output was INVALID.\n",
    "\n",
    "Your job now:\n",
    "- Output ONLY a valid JSON object (no markdown, no extra text).\n",
    "- The JSON must contain EXACTLY these keys with correct types:\n",
    "  - overall_score (integer 1-5)\n",
    "  - rationale (string)\n",
    "  - detailed_feedback (array of strings)\n",
    "  - concerns (array of strings)\n",
    "  - recommended_action (string)\n",
    "\n",
    "Fix any formatting issues and ensure the schema is correct.\n",
    "\"\"\"\n",
    "\n",
    "        repair_messages = [\n",
    "            SystemMessage(content=LEGAL_SURVEY_RUBRIC),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"{repair_instructions}\\n\\n\"\n",
    "                    \"LEGAL SURVEY QUESTION:\\n\"\n",
    "                    f\"{survey_question}\\n\\n\"\n",
    "                    \"YOUR PREVIOUS (INVALID) OUTPUT:\\n\"\n",
    "                    f\"{raw}\\n\\n\"\n",
    "                    \"Return ONLY valid JSON.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        resp2, latency_ms2 = timed_invoke(repair_messages)\n",
    "        raw2 = (resp2.content or \"\").strip()\n",
    "        parsed2 = _extract_json_object(raw2)\n",
    "\n",
    "        if parsed2 is not None:\n",
    "            errs2 = validate_judge_output(parsed2)\n",
    "            if not errs2:\n",
    "                if print_debug:\n",
    "                    print(f\"✅ Repair OK (attempt {attempt}). Latency: {latency_ms2:.2f} ms\")\n",
    "                return parsed2\n",
    "            if print_debug:\n",
    "                print(f\"⚠️ Repair schema still invalid (attempt {attempt}). Latency: {latency_ms2:.2f} ms\")\n",
    "                for e in errs2:\n",
    "                    print(\"-\", e)\n",
    "        else:\n",
    "            if print_debug:\n",
    "                print(f\"⚠️ Repair JSON parse failed (attempt {attempt}). Latency: {latency_ms2:.2f} ms\")\n",
    "                print(\"Raw repair response:\\n\", raw2[:1200])\n",
    "\n",
    "        raw = raw2\n",
    "        attempt += 1\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example: run with repair enabled\n",
    "result = judge_with_repair(good_legal_survey_question, max_attempts=2, print_debug=True)\n",
    "pretty_print(\"GOOD question (judge_with_repair)\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3f65ac-1685-43c9-9873-c1422550d83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n  \"trace_id\": \"ad305c37-8257-4d30-82cc-75955feefbc0\",\n  \"timestamp_utc\": \"2026-01-16T20:42:01.856302Z\",\n  \"endpoint\": \"databricks-meta-llama-3-1-8b-instruct\",\n  \"latency_ms\": 1556.6365420017974,\n  \"prompt_chars\": 2393,\n  \"response_chars\": 1310,\n  \"prompt_tokens_approx\": 359,\n  \"response_tokens_approx\": 171,\n  \"repaired\": false,\n  \"initial_schema_errors\": [],\n  \"judge_output\": {\n    \"overall_score\": 4,\n    \"rationale\": \"The question is well-structured and clear, but it could benefit from a more precise definition of 'emerging AI-generated content' to ensure respondents understand the scope.\",\n    \"detailed_feedback\": [\n      \"Clarity & Precision: The question is mostly clear, but the term 'emerging AI-generated content' could be more precisely defined.\",\n      \"Neutrality & Bias: The question is generally neutral, but the assumption that the content meets the originality criteria might subtly influence respondents.\",\n      \"Relevance & Focus: The question is highly relevant to the survey's topic and well-focused on a single concept.\",\n      \"Completeness: The question provides all necessary context and information for the respondent to answer thoroughly.\",\n      \"Appropriateness for Audience: The question is generally appropriate for the assumed legal knowledge and background of the target survey audience.\"\n    ],\n    \"concerns\": [\n      \"The question's assumption about the content meeting the originality criteria might influence respondents.\",\n      \"The term 'emerging AI-generated content' could be more precisely defined.\"\n    ],\n    \"recommended_action\": \"Consider adding a brief definition or clarification of 'emerging AI-generated content' to ensure respondents understand the scope.\"\n  },\n  \"raw_output\": \"{\\n  \\\"overall_score\\\": 4,\\n  \\\"rationale\\\": \\\"The question is well-structured and clear, but it could benefit from a more precise definition of 'emerging AI-generated content' to ensure respondents understand the scope.\\\",\\n  \\\"detailed_feedback\\\": [\\n    \\\"Clarity & Precision: The question is mostly clear, but the term 'emerging AI-generated content' could be more precisely defined.\\\",\\n    \\\"Neutrality & Bias: The question is generally neutral, but the assumption that the content meets the originality criteria might subtly influence respondents.\\\",\\n    \\\"Relevance & Focus: The question is highly relevant to the survey's topic and well-focused on a single concept.\\\",\\n    \\\"Completeness: The question provides all necessary context and information for the respondent to answer thoroughly.\\\",\\n    \\\"Appropriateness for Audience: The question is generally appropriate for the assumed legal knowledge and background of the target survey audience.\\\"\\n  ],\\n  \\\"concerns\\\": [\\n    \\\"The question's assumption about the content meeting the originality criteria might influence respondents.\\\",\\n    \\\"The term 'emerging AI-generated content' could be more precisely defined.\\\"\\n  ],\\n  \\\"recommended_action\\\": \\\"Consider adding a brief definition or clarification of 'emerging AI-generated content' to ensure respondents understand the scope.\\\"\\n}\"\n}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Add observability: latency + (approx) token counts + structured record\n",
    "\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def approx_tokens(text: str) -> int:\n",
    "    # Placeholder token estimator (words). Replace with real token usage if your stack exposes it.\n",
    "    return len((text or \"\").split())\n",
    "\n",
    "def run_judge_observed(survey_question: str) -> Dict[str, Any]:\n",
    "    trace_id = str(uuid.uuid4())\n",
    "    ts = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    # Build messages and time the call\n",
    "    messages = judge._build_messages(survey_question)\n",
    "    resp, latency_ms = timed_invoke(messages)\n",
    "    raw = (resp.content or \"\").strip()\n",
    "\n",
    "    parsed = _extract_json_object(raw)\n",
    "    final = parsed\n",
    "    repaired = False\n",
    "    schema_errors = None\n",
    "\n",
    "    if final is None:\n",
    "        final = judge_with_repair(survey_question, max_attempts=2, print_debug=False)\n",
    "        repaired = True\n",
    "    else:\n",
    "        schema_errors = validate_judge_output(final)\n",
    "        if schema_errors:\n",
    "            final = judge_with_repair(survey_question, max_attempts=2, print_debug=False)\n",
    "            repaired = True\n",
    "\n",
    "    record = {\n",
    "        \"trace_id\": trace_id,\n",
    "        \"timestamp_utc\": ts,\n",
    "        \"endpoint\": LLM_ENDPOINT_NAME,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"prompt_chars\": sum(len(m.content) for m in messages),\n",
    "        \"response_chars\": len(raw),\n",
    "        \"prompt_tokens_approx\": sum(approx_tokens(m.content) for m in messages),\n",
    "        \"response_tokens_approx\": approx_tokens(raw),\n",
    "        \"repaired\": repaired,\n",
    "        \"initial_schema_errors\": schema_errors,\n",
    "        \"judge_output\": final,\n",
    "        \"raw_output\": raw[:2000],  # keep capped for logs\n",
    "    }\n",
    "    return record\n",
    "\n",
    "\n",
    "# Example observed run\n",
    "observed = run_judge_observed(good_legal_survey_question)\n",
    "print(json.dumps(observed, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee4ce9e7-dd39-44b1-bd57-afe8315ca7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}