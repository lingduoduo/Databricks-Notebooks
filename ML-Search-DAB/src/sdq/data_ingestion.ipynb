{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f5820e-5806-4d32-b927-c958a6aad902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "logs_table = pd.read_json(\"data/sdq_test_data_2025-02-14.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd5080d-6b76-4b6a-a461-952fbd716fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import ast\n",
    "\n",
    "sample = ast.literal_eval(base64.b64decode(logs_table.iloc[0][\"captureData\"][\"endpointInput\"]['data']).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdq = spark.createDataFrame(logs_table[\"captureData\"].apply(lambda x: x[\"endpointInput\"][\"data\"]), [\"encoded_list\"])\n",
    "display(df_sdq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_sdq_db = dbutils.widgets.get(\"ml_sdq_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Define the schema for the JSON objects\n",
    "schema = StructType([\n",
    "                StructField('session_id', StringType(), nullable=False), \n",
    "                StructField('timestamp',  StringType(), nullable=False),\n",
    "                StructField('client_id',  StringType(), nullable=False),\n",
    "                StructField('associate_id',  StringType(), nullable=False),\n",
    "                StructField('trace_id',  StringType(), nullable=False),\n",
    "                StructField('dq_id',  StringType(), nullable=False),\n",
    "                StructField('params',  StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Explode the list of JSON objects\n",
    "res = df_sdq.withColumn(\"encoded_list_extract_sdq\", expr(f\"{ml_catalog}.{ml_sdq_db}.literal_eval_sdq(encoded_list)\"))\n",
    "exploded_df = res.withColumn(\"decoded_list\", F.explode(F.col(\"encoded_list_extract_sdq\")))\n",
    "parsed_df = exploded_df.withColumn(\"decoded_list\", F.from_json(F.col(\"decoded_list\"), schema))\n",
    "final_df = parsed_df.select(*[col for col in res.columns if col != \"encoded_list_extract_sdq\"], \"decoded_list.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "yesterday = F.date_sub(F.current_date(), 1)\n",
    "\n",
    "# Add year, month, and day columns to the DataFrame\n",
    "final_df = final_df.withColumn(\"year\", F.year(F.lit(yesterday))) \\\n",
    "                   .withColumn(\"month\", F.year(F.lit(yesterday))) \\\n",
    "                   .withColumn(\"day\", F.year(F.lit(yesterday)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_sdq_db}.lifion_cacheable_dq_sample\")\n",
    "\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\", \"month\", \"day\").saveAsTable(f\"{ml_catalog}.{ml_sdq_db}.lifion_cacheable_dq_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "\n",
    "if secret_scope.split(\"-\")[0] == \"dit\":\n",
    "    dbutils.notebook.exit(\"No need to run the following code in DIT environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Investigate the data in Production"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%md\n",
    "\n",
    "### Investigate the data in Production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_table = spark.sql(\"select * from nas_raw_lifion_prod.lifion_logs_for_sdq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(logs_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sagemaker_data = spark.read.table(\"nas_raw_lifion_prod.lifion_sagemaker_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sagemaker_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_sagemaker_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sagemaker_data = spark.read.table(\"nas_raw_lifion_prod.lifion_sagemaker_raw_prod_use1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sagemaker_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_sagemaker_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_only_data_table = spark.read.table(\"nas_raw_lifion_prod.lifion_sdq_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_only_data_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dq_only_data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_only_data_table = spark.read.table(\"nas_raw_lifion_prod.lifion_sdq_params_prod_use1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_only_data_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dq_only_data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheable_dq = spark.read.table(\"nas_raw_lifion_prod.lifion_cacheable_dq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheable_dq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cacheable_dq)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4498369276804650,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2024-12-04 Test Notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "python11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
