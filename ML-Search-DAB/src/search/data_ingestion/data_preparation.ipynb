{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search = spark.sql(f\"\"\"select \n",
    "    user_agent,\n",
    "    uuid1_time,\n",
    "    _token_associate_id,\n",
    "    label,\n",
    "    _token_client_id,\n",
    "    _token_session_id,\n",
    "    context,\n",
    "    athena_tablename,\n",
    "    technical_mini_app_version,\n",
    "    technical_mini_app,\n",
    "    _token_person_id,\n",
    "    action,\n",
    "    os,\n",
    "    browser,\n",
    "    schema_version,\n",
    "    _token_mask,\n",
    "    date_key,\n",
    "    client_id,\n",
    "    tile_id,\n",
    "    category,\n",
    "    timezone,\n",
    "    event_id,\n",
    "    time_stamp,\n",
    "    request_correlation_id,\n",
    "    details_search_value,\n",
    "    details_search_results,\n",
    "    year(date_sub(current_date(), 1)) as year,\n",
    "    month(date_sub(current_date(), 1)) as month,\n",
    "    day(date_sub(current_date(), 1)) as day\n",
    "from {ml_catalog}.{ml_search_db}.ml_search where details_search_results is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the JSON objects\n",
    "schema = StructType([\n",
    "    StructField('_id', StringType(), True),\n",
    "    StructField('finalScore', FloatType(), True),\n",
    "    StructField('resPos', IntegerType(), True),\n",
    "    StructField('traceId', StringType(), True),\n",
    "    StructField('queryId', StringType(), True)\n",
    "])\n",
    "\n",
    "# Explode the list of JSON objects\n",
    "res = df_search.withColumn(\"details_search_results_extract_search\", expr(f\"{ml_catalog}.{ml_search_db}.literal_eval_search(details_search_results)\"))\n",
    "exploded_df = res.withColumn(\"details\", F.explode(F.col(\"details_search_results_extract_search\")))\n",
    "parsed_df = exploded_df.withColumn(\"details\", F.from_json(F.col(\"details\"), schema))\n",
    "final_df = parsed_df.select(*[col for col in res.columns if col != \"details_search_results_extract_search\"], \"details.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_all\")\n",
    "\n",
    "(final_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = spark.sql(f\"\"\"select \n",
    "    user_agent,\n",
    "    uuid1_time,\n",
    "    _token_associate_id,\n",
    "    label,\n",
    "    _token_client_id,\n",
    "    _token_session_id,\n",
    "    context,\n",
    "    athena_tablename,\n",
    "    technical_mini_app_version,\n",
    "    technical_mini_app,\n",
    "    _token_person_id,\n",
    "    action,\n",
    "    os,\n",
    "    browser,\n",
    "    schema_version,\n",
    "    _token_mask,\n",
    "    date_key,\n",
    "    client_id,\n",
    "    tile_id,\n",
    "    category,\n",
    "    timezone,\n",
    "    event_id,\n",
    "    time_stamp,\n",
    "    request_correlation_id,\n",
    "    details_search_value,\n",
    "    details_search_results,\n",
    "    year(date_sub(current_date(), 1)) as year,\n",
    "    month(date_sub(current_date(), 1)) as month,\n",
    "    day(date_sub(current_date(), 1)) as day\n",
    "from {ml_catalog}.{ml_search_db}.ml_search where action = 'people' and details_search_results is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the JSON objects\n",
    "schema = StructType([\n",
    "    StructField('_id', StringType(), True),\n",
    "    StructField('legalName', StringType(), True),\n",
    "    StructField('displayName', StringType(), True),\n",
    "    StructField('eID', StringType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('position', StringType(), True),\n",
    "    StructField('finalScore', FloatType(), True),\n",
    "    StructField('resPos', IntegerType(), True),\n",
    "    StructField('traceId', StringType(), True),\n",
    "    StructField('queryId', StringType(), True)\n",
    "])\n",
    "\n",
    "# Explode the list of JSON objects\n",
    "res = df_people.withColumn(\"details_search_results_extract_people\", expr(f\"{ml_catalog}.{ml_search_db}.literal_eval_people(details_search_results)\"))\n",
    "exploded_df = res.withColumn(\"details\", F.explode(F.col(\"details_search_results_extract_people\")))\n",
    "parsed_df = exploded_df.withColumn(\"details\", F.from_json(F.col(\"details\"), schema))\n",
    "final_df = parsed_df.select(*[col for col in res.columns if col != \"details_search_results_extract_people\"], \"details.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_people\")\n",
    "\n",
    "(final_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_people\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_action = spark.sql(f\"\"\"select \n",
    "    user_agent,\n",
    "    uuid1_time,\n",
    "    _token_associate_id,\n",
    "    label,\n",
    "    _token_client_id,\n",
    "    _token_session_id,\n",
    "    context,\n",
    "    athena_tablename,\n",
    "    technical_mini_app_version,\n",
    "    technical_mini_app,\n",
    "    _token_person_id,\n",
    "    action,\n",
    "    os,\n",
    "    browser,\n",
    "    schema_version,\n",
    "    _token_mask,\n",
    "    date_key,\n",
    "    client_id,\n",
    "    tile_id,\n",
    "    category,\n",
    "    timezone,\n",
    "    event_id,\n",
    "    time_stamp,\n",
    "    request_correlation_id,\n",
    "    details_search_value,\n",
    "    details_search_results,\n",
    "    year(date_sub(current_date(), 1)) as year,\n",
    "    month(date_sub(current_date(), 1)) as month,\n",
    "    day(date_sub(current_date(), 1)) as day\n",
    "from {ml_catalog}.{ml_search_db}.ml_search where action = 'actions' and details_search_results is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the JSON objects\n",
    "schema = StructType([\n",
    "    StructField('_id', StringType(), True),\n",
    "    StructField('caption', StringType(), True),\n",
    "    StructField('subtitle', StringType(), True),\n",
    "    StructField('solrScore', FloatType(), True),  \n",
    "    StructField('finalScore', FloatType(), True),\n",
    "    StructField('scoreDistributionCaption', FloatType(), True), \n",
    "    StructField('scoreDistributionDescription', FloatType(), True), \n",
    "    StructField('scoreDistributionKeywords', FloatType(), True), \n",
    "    StructField('resPos', IntegerType(), True),\n",
    "    StructField('traceId', StringType(), True),\n",
    "    StructField('queryId', StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Explode the list of JSON objects\n",
    "res = df_action.withColumn(\"details_search_results_extract_action\", expr(f\"{ml_catalog}.{ml_search_db}.literal_eval_action(details_search_results)\"))\n",
    "exploded_df = res.withColumn(\"details\", F.explode(F.col(\"details_search_results_extract_action\")))\n",
    "parsed_df = exploded_df.withColumn(\"details\", F.from_json(F.col(\"details\"), schema))\n",
    "final_df = parsed_df.select(*[col for col in res.columns if col != \"details_search_results_extract_action\"], \"details.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_action\")\n",
    "\n",
    "(final_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_action\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Searches and Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.sql(f\"\"\"\n",
    "with click as (\n",
    "    select request_correlation_id,\n",
    "        _token_session_id, \n",
    "        _token_associate_id, \n",
    "        object_id, \n",
    "        time_stamp, \n",
    "        label, \n",
    "        client_id, \n",
    "        category, \n",
    "        details_caption \n",
    "    from {ml_catalog}.{ml_search_db}.ml_search_click \n",
    "    where request_correlation_id is not null and lower(request_correlation_id) != 'nan'\n",
    "),\n",
    "\n",
    "search_click as (\n",
    "        select search.request_correlation_id,\n",
    "        search.resPos,\n",
    "        search.traceId,\n",
    "        rank() over (partition by search.request_correlation_id, search.resPos, search.traceId order by click.time_stamp) as rank,\n",
    "        click._token_session_id as click_session_id, \n",
    "        click._token_associate_id as click_associate_id, \n",
    "        click.object_id as click_object_id, \n",
    "        click.time_stamp as click_time_stamp, \n",
    "        click.label as click_label, \n",
    "        click.client_id as click_client_id, \n",
    "        click.category as click_category, \n",
    "        click.details_caption as click_details_caption\n",
    "    from {ml_catalog}.{ml_search_db}.ml_search_all search\n",
    "    inner join click\n",
    "    on search.request_correlation_id = click.request_correlation_id\n",
    "    and search._id = click.object_id\n",
    ")\n",
    "\n",
    "select search.*, \n",
    "        search_click.click_session_id, \n",
    "        search_click.click_associate_id, \n",
    "        search_click.click_object_id, \n",
    "        search_click.click_time_stamp, \n",
    "        search_click.click_label, \n",
    "        search_click.click_client_id, \n",
    "        search_click.click_category, \n",
    "        search_click.click_details_caption,\n",
    "        if(search_click.traceId is null, 0, 1) as click\n",
    "from {ml_catalog}.{ml_search_db}.ml_search_all search\n",
    "left join (\n",
    "    select *\n",
    "    from search_click\n",
    "    where rank = 1\n",
    ") search_click\n",
    "on search.traceId = search_click.traceId and search.resPos = search_click.resPos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_with_click\")\n",
    "\n",
    "(final_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_with_click\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
