{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "if config['injection_schedule']['is_weekly']:\n",
    "    date_range = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f5820e-5806-4d32-b927-c958a6aad902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the date range for the last 7 days\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add in correlationid to join search and click, tracking from 3/18/2025\n",
    "# Click data has issues after 2025/4/19 (inclusive)\n",
    "end_date = datetime.now() - timedelta(days=1)\n",
    "# end_date = datetime(2025, 4, 18)\n",
    "start_date = max(datetime(2025, 3, 18), end_date - timedelta(days=date_range))\n",
    "date_range = (end_date - start_date).days\n",
    "print(f\"Start date: {start_date}\")\n",
    "print(f\"End date: {end_date}\")\n",
    "print(f\"Date range: {date_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a864cf79-b061-469b-8bc9-578f5dcc3b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Generate the list of file paths in the date range\n",
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")\n",
    "ml_search_volume = dbutils.widgets.get(\"ml_search_volume\")\n",
    "\n",
    "base_path = f\"/Volumes/{ml_catalog}/{ml_search_db}/{ml_search_volume}\"\n",
    "file_paths = []\n",
    "for single_date in (start_date + timedelta(n) for n in range(date_range)):\n",
    "    year = single_date.strftime(\"%Y\")\n",
    "    month = single_date.strftime(\"%m\")\n",
    "    day = single_date.strftime(\"%d\")\n",
    "    file_path = f\"{base_path}/year={year}/month={month}/day={day}/\"\n",
    "    # Check if the folder exists before appending\n",
    "    if os.path.exists(os.path.dirname(file_path)):\n",
    "        file_paths.append(file_path+\"*/*.parquet\")\n",
    "    else:\n",
    "        print(f\"Folder does not exist: {os.path.dirname(file_path)}\")\n",
    "else:\n",
    "    year = end_date.strftime(\"%Y\")\n",
    "    month = end_date.strftime(\"%m\")\n",
    "    day = end_date.strftime(\"%d\")\n",
    "    file_path = f\"{base_path}/year={year}/month={month}/day={day}/\"\n",
    "    # Check if the folder exists before appending\n",
    "    if os.path.exists(os.path.dirname(file_path)):\n",
    "        file_paths.append(file_path+\"*/*.parquet\")\n",
    "    else:\n",
    "        print(f\"Folder does not exist: {os.path.dirname(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a4a82f-54bb-4f8e-8b40-2acf5c3ee545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_agent\", StringType(), True),\n",
    "    StructField(\"uuid1_time\", StringType(), True),\n",
    "    StructField(\"_token_associate_id\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"_token_client_id\", StringType(), True),\n",
    "    StructField(\"object_id\", StringType(), True),\n",
    "    StructField(\"_token_session_id\", StringType(), True),\n",
    "    StructField(\"context\", StringType(), True),\n",
    "    StructField(\"athena_tablename\", StringType(), True),\n",
    "    StructField(\"time_stamp\", StringType(), True),\n",
    "    StructField(\"_token_person_id\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"os\", StringType(), True),\n",
    "    StructField(\"browser\", StringType(), True),\n",
    "    StructField(\"schema_version\", StringType(), True),\n",
    "    StructField(\"_token_mask\", StringType(), True),\n",
    "    StructField(\"date_key\", StringType(), True),\n",
    "    StructField(\"client_id\", StringType(), True),\n",
    "    StructField(\"tile_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"timezone\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"details_caption\", StringType(), True),\n",
    "    StructField(\"technical_mini_app_version\", StringType(), True),\n",
    "    StructField(\"technical_mini_app\", StringType(), True),\n",
    "    StructField(\"request_correlation_id\", StringType(), True),\n",
    "    StructField(\"details_search_value\", StringType(), True),\n",
    "    StructField(\"details_search_results\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"parquet\").schema(schema).load(file_paths)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd5080d-6b76-4b6a-a461-952fbd716fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search\")\n",
    "\n",
    "(df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Generate the list of file paths in the date range\n",
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")\n",
    "ml_search_click_volume = dbutils.widgets.get(\"ml_search_click_volume\")\n",
    "\n",
    "base_path = f\"/Volumes/{ml_catalog}/{ml_search_db}/{ml_search_click_volume}\"\n",
    "file_paths = []\n",
    "for single_date in (start_date + timedelta(n) for n in range(date_range)):\n",
    "    year = single_date.strftime(\"%Y\")\n",
    "    month = single_date.strftime(\"%m\")\n",
    "    day = single_date.strftime(\"%d\")\n",
    "    file_path = f\"{base_path}/year={year}/month={month}/day={day}/\"\n",
    "    # Check if the folder exists before appending\n",
    "    if os.path.exists(os.path.dirname(file_path)):\n",
    "        file_paths.append(file_path+\"*/*.parquet\")\n",
    "    else:\n",
    "        print(f\"Folder does not exist: {os.path.dirname(file_path)}\")\n",
    "else:\n",
    "    year = end_date.strftime(\"%Y\")\n",
    "    month = end_date.strftime(\"%m\")\n",
    "    day = end_date.strftime(\"%d\")\n",
    "    file_path = f\"{base_path}/year={year}/month={month}/day={day}/\"\n",
    "    # Check if the folder exists before appending\n",
    "    if os.path.exists(os.path.dirname(file_path)):\n",
    "        file_paths.append(file_path+\"*/*.parquet\")\n",
    "    else:\n",
    "        print(f\"Folder does not exist: {os.path.dirname(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_agent\", StringType(), True),\n",
    "    StructField(\"uuid1_time\", StringType(), True),\n",
    "    StructField(\"_token_associate_id\", StringType(), True),\n",
    "    StructField(\"_token_client_id\", StringType(), True),\n",
    "    StructField(\"details_behavior_instance_id\", StringType(), True),\n",
    "    StructField(\"details_tile_id\", StringType(), True),\n",
    "    StructField(\"details_br_count\", StringType(), True),\n",
    "    StructField(\"details_correlation_id\", StringType(), True),\n",
    "    StructField(\"details_trace_ids\", StringType(), True),\n",
    "    StructField(\"_token_session_id\", StringType(), True),\n",
    "    StructField(\"athena_tablename\", StringType(), True),\n",
    "    StructField(\"details_block_id\", StringType(), True),\n",
    "    StructField(\"details_associate_id\", StringType(), True),\n",
    "    StructField(\"details_timestamp\", StringType(), True),\n",
    "    StructField(\"_token_person_id\", StringType(), True),\n",
    "    StructField(\"details_error\", StringType(), True),\n",
    "    StructField(\"details_value\", StringType(), True),\n",
    "    StructField(\"os\", StringType(), True),\n",
    "    StructField(\"browser\", StringType(), True),\n",
    "    StructField(\"schema_version\", StringType(), True),\n",
    "    StructField(\"_token_mask\", StringType(), True),\n",
    "    StructField(\"date_key\", StringType(), True),\n",
    "    StructField(\"client_id\", StringType(), True),\n",
    "    StructField(\"details_business_rule_id\", StringType(), True),\n",
    "    StructField(\"details_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"timezone\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"type_id\", StringType(), True),\n",
    "    StructField(\"details_object_id\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"details_context\", StringType(), True),\n",
    "    StructField(\"details_type_id\", StringType(), True),\n",
    "    StructField(\"object_id\", StringType(), True),\n",
    "    StructField(\"caption\", StringType(), True),\n",
    "    StructField(\"context\", StringType(), True),\n",
    "    StructField(\"technical_mini_app_version\", StringType(), True),\n",
    "    StructField(\"technical_mini_app\", StringType(), True),\n",
    "    StructField(\"details_technical_mini_app\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"non_interaction\", StringType(), True),\n",
    "    StructField(\"tile_id\", StringType(), True),\n",
    "    StructField(\"details_caption\", StringType(), True),\n",
    "    StructField(\"details_technical_mini_app_version\", StringType(), True),\n",
    "    StructField(\"details_tab_block_id\", StringType(), True),\n",
    "    StructField(\"details_opened_as\", StringType(), True),\n",
    "    StructField(\"details_tab_block_name\", StringType(), True),\n",
    "    StructField(\"details_is_impure\", StringType(), True),\n",
    "    StructField(\"details_variable\", StringType(), True),\n",
    "    StructField(\"details_tile_caption\", StringType(), True),\n",
    "    StructField(\"details_page_load_id\", StringType(), True),\n",
    "    StructField(\"details_modal_caption\", StringType(), True),\n",
    "    StructField(\"details_modal_id\", StringType(), True),\n",
    "    StructField(\"time_stamp\", StringType(), True),\n",
    "    StructField(\"business_mini_app_versions\", StringType(), True),\n",
    "    StructField(\"business_mini_apps\", StringType(), True),\n",
    "    StructField(\"request_correlation_id\", StringType(), True),\n",
    "    StructField(\"details_search_value\", StringType(), True),\n",
    "    StructField(\"details_search_results\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"parquet\").schema(schema).load(file_paths)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_click\")\n",
    "\n",
    "(df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_click\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile collected sparse data:\n",
    "end_date = datetime(2025, 2, 19)\n",
    "print(f\"End date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the monthly date\n",
    "# Generate the list of file paths in the date range\n",
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")\n",
    "ml_search_mobile_events_volume = dbutils.widgets.get(\"ml_search_mobile_events_volume\")\n",
    "\n",
    "base_path = f\"/Volumes/{ml_catalog}/{ml_search_db}/{ml_search_mobile_events_volume}\"\n",
    "\n",
    "year = end_date.strftime(\"%Y\")\n",
    "month = end_date.strftime(\"%m\")\n",
    "file_path = f\"{base_path}/year={year}/month={month}/*/*/*.parquet\"\n",
    "file_paths.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(file_paths)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_mobile\")\n",
    "\n",
    "(df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_mobile\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4498369276804650,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2024-12-04 Test Notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "python11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
