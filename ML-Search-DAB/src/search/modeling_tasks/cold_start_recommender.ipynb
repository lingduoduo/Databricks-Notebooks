{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c22ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_actions = spark.sql(f\"\"\"WITH clicks AS (\n",
    "    SELECT\n",
    "        client_id,\n",
    "        click_object_id,\n",
    "        click_details_caption,\n",
    "        TO_UNIX_TIMESTAMP(time_stamp, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") AS unix_timestamp,\n",
    "        COUNT(*) AS clicks\n",
    "    FROM\n",
    "        {ml_catalog}.{ml_search_db}.ml_search_with_click\n",
    "    WHERE\n",
    "        click_object_id IS NOT NULL \n",
    "        AND action = 'actions'\n",
    "    GROUP BY\n",
    "        client_id,\n",
    "        click_object_id,\n",
    "        click_details_caption,\n",
    "        TO_UNIX_TIMESTAMP(time_stamp, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    "),\n",
    "\n",
    "clicks_with_max AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        MAX(unix_timestamp) OVER () AS max_timestamp\n",
    "    FROM clicks\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    client_id,\n",
    "    click_object_id,\n",
    "    click_details_caption,\n",
    "    SUM((1.0 / (1 + ((max_timestamp - unix_timestamp) / (24 * 60 * 60 * 100)))) * clicks) AS weighted_clicks\n",
    "FROM\n",
    "    clicks_with_max\n",
    "GROUP BY\n",
    "    client_id,\n",
    "    click_object_id,\n",
    "    click_details_caption\n",
    "ORDER BY\n",
    "    client_id,\n",
    "    weighted_clicks DESC;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdf25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(recommended_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707267f6",
   "metadata": {},
   "source": [
    "### Export to Spark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.cold_start_recommended_actions\")\n",
    "\n",
    "(recommended_actions\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.cold_start_recommended_actions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "\n",
    "if secret_scope.split(\"-\")[0] == \"prod\":\n",
    "    dbutils.notebook.exit(\"Skip run in prod environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e2163",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "boto3_session = boto3.Session(\n",
    "    botocore_session=dbutils.credentials.getServiceCredentialsProvider(\n",
    "        'service-cred-nas-lifion_ml-sdq-dit'\n",
    "    )\n",
    ")\n",
    "s3_client = boto3_session.client('s3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8aa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "bucket_name = \"ml-models-bucket-appbuild-02\"\n",
    "ts = datetime.now()\n",
    "\n",
    "file = f\"cold_start_{ts}.csv\"\n",
    "recommended_actions.toPandas().to_csv(file, index=False)\n",
    "\n",
    "# Upload file to S3\n",
    "file_path = f\"recommended-actions/{file}\"\n",
    "response = s3_client.put_object(Bucket=bucket_name, Body=open(file, \"rb\"), Key=file_path)\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 put_object response. Status - {status}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
