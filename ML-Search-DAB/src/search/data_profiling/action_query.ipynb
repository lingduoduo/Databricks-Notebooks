{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")\n",
    "\n",
    "df_action = spark.sql(f\"\"\"\n",
    "    SELECT _id, caption, subtitle, count(*) AS views\n",
    "    FROM {ml_catalog}.{ml_search_db}.ml_search_action\n",
    "    WHERE context='US'\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 4 DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top Search Action Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenize and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_action = df_action.toPandas()\n",
    "pdf_action['combined'] = pdf_action.apply(\n",
    "    lambda x: x.caption if x.caption.lower() == x.subtitle.lower() else ','.join([x.caption, x.subtitle]), \n",
    "    axis=1\n",
    ")\n",
    "documents = \"\\n\\n\".join(pdf_action.combined)\n",
    "documents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.utils.data_profiling_nlp import CustomTextSplitter\n",
    "\n",
    "splitter = CustomTextSplitter(separator=\"\\n\\n\")\n",
    "processed_tokens = splitter.split_text(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "words = ','.join(processed_tokens)\n",
    "w = words.split(',')\n",
    "\n",
    "most_common_keywords = Counter(w).most_common(100)\n",
    "sorted_keywords = sorted(most_common_keywords, key=lambda x: x[0])\n",
    "top_keywords = pd.DataFrame(sorted_keywords, columns=['keyword', 'count'])\n",
    "display(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import nltk\n",
    "\n",
    "# Compute frequency distribution and get top K\n",
    "keyword_chunks: List[str] = []\n",
    "for tokens in processed_tokens:\n",
    "    token_list = tokens.split(\",\")\n",
    "    freq_dist = nltk.FreqDist(token_list)\n",
    "    top_keywords = [word for word, _ in freq_dist.most_common(5)]\n",
    "    keyword_chunks.append(\",\".join(top_keywords))\n",
    "pdf_action['keywords'] = keyword_chunks\n",
    "pdf_action.drop(['combined'], axis=1, inplace=True)\n",
    "display(pdf_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keywords by BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = processed_tokens\n",
    "print(f\"--{passages[0]}\\n--{passages[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.split(','):\n",
    "        token = token.strip(string.punctuation)\n",
    "        if len(token) > 0:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in passages:\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "# #Create a BM25 index from the tokenized document corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def search(query, top_k=3, num_candidates=100):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop-3 lexical search (BM25) hits\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'],passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"dashboard\", top_k=3, num_candidates=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df_action = df_action.withColumn(\n",
    "    \"combined\", \n",
    "    concat_ws(\" | \", df_action[\"caption\"], df_action[\"subtitle\"])\n",
    ")\n",
    "# df = df_action.select(\"combined\").dropDuplicates()\n",
    "# pdf = df.sample(fraction=0.1).toPandas()\n",
    "pdf = df_action.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.utils.data_profiling_llm import get_bearer_token\n",
    "\n",
    "client_secret = dbutils.widgets.get(\"client_secret\")\n",
    "bearer_token = get_bearer_token(client_secret)\n",
    "print(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from search.utils.data_profiling_llm import get_openai_embedding\n",
    "\n",
    "embed_start_time = time.time()\n",
    "\n",
    "pdf[\"embedding\"] = pdf.combined.apply(lambda x: get_openai_embedding(client_secret, x))\n",
    "\n",
    "embed_time = time.time() - embed_start_time\n",
    "print(f\"Embedding took {embed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pdf['embedding'] = pdf['embedding'].apply(lambda x: np.array(x).astype(np.float32).tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "yesterday = F.date_sub(F.current_date(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_action_embed\")\n",
    "\n",
    "\n",
    "# Add year, month, and day columns to the DataFrame\n",
    "spark_df = spark.createDataFrame(pdf).withColumn(\"year\", F.year(F.lit(yesterday))) \\\n",
    "                   .withColumn(\"month\", F.year(F.lit(yesterday))) \\\n",
    "                   .withColumn(\"day\", F.year(F.lit(yesterday)))\n",
    "\n",
    "(spark_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_action_embed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "\n",
    "if secret_scope.split(\"-\")[0] == \"prod\":\n",
    "    dbutils.notebook.exit(\"Skip run in prod environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redis index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rvl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "# Redis connection params\n",
    "REDIS_HOST = 'search01d.us.caas.oneadp.com'\n",
    "REDIS_PORT = 443\n",
    "REDIS_PASSWORD = dbutils.widgets.get(\"redis_secret\")\n",
    "\n",
    "# Create Redis client\n",
    "redis_client = redis.Redis(\n",
    "  host=REDIS_HOST,\n",
    "  port=REDIS_PORT,\n",
    "  password=REDIS_PASSWORD,\n",
    "  decode_responses=True, \n",
    "  ssl_cert_reqs=\"none\", \n",
    "  ssl=True)\n",
    "# Test connection\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis_client.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.schema import IndexSchema\n",
    "from redisvl.index import SearchIndex\n",
    "\n",
    "index_name = \"action_links\"\n",
    "\n",
    "schema = IndexSchema.from_dict({\n",
    "  \"index\": {\n",
    "    \"name\": index_name,\n",
    "    \"prefix\": index_name,\n",
    "    \"storage_type\": \"hash\"\n",
    "  },\n",
    "  \"fields\": [\n",
    "    {\"type\" : \"tag\", \"name\" : \"caption\", \"attrs\": {\"sortable\": True}},\n",
    "    {\"type\" : \"text\", \"name\" : \"subtitle\"},\n",
    "    {\"type\" : \"text\", \"name\" : \"combined\"},\n",
    "    {\"type\" : \"numeric\", \"name\" : \"views\", \"attrs\": {\"sortable\": True}},\n",
    "    {\n",
    "        \"type\" : \"vector\",\n",
    "        \"name\" : \"embedding\",\n",
    "        \"attrs\" : {\n",
    "            \"dims\": 3072,\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"algorithm\": \"flat\",\n",
    "            \"datatype\": \"float32\"\n",
    "        }\n",
    "    }\n",
    "  ],\n",
    "})\n",
    "\n",
    "index = SearchIndex(schema, redis_client)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.load(pdf.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client.dbsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Search Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Mana\"\n",
    "vector = get_openai_embedding(client_secret, user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query import VectorQuery\n",
    "\n",
    "vec_query = VectorQuery(\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    num_results=5,\n",
    "    return_fields=[\"caption\", \"subtitle\"],\n",
    "    return_score=True,\n",
    ")\n",
    "\n",
    "result = index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query.filter import Tag\n",
    "\n",
    "tag_filter = Tag(\"caption\") == \"Asset Management\"\n",
    "\n",
    "vec_query.set_filter(tag_filter)\n",
    "\n",
    "result = index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query.filter import Num\n",
    "\n",
    "# build combined filter expressions\n",
    "tag_filter = Tag(\"caption\") == \"Asset Management\"\n",
    "num_filter = Num(\"views\") >= 2\n",
    "combined_filter = tag_filter & num_filter\n",
    "\n",
    "# build vector query\n",
    "vec_query = VectorQuery(\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    num_results=5,\n",
    "    return_fields=[\"caption\", \"subtitle\"],\n",
    "    return_score=True,\n",
    "    filter_expression=combined_filter\n",
    ")\n",
    "\n",
    "result=index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query.filter import Text\n",
    "\n",
    "text_filter = Text(\"subtitle\") % \"Asset Management\"\n",
    "\n",
    "vec_query = VectorQuery(\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    num_results=3,\n",
    "    return_fields=[\"caption\", \"subtitle\"],\n",
    "    return_score=True,\n",
    "    filter_expression=text_filter\n",
    ")\n",
    "\n",
    "result = index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_filter =  Text(\"combined\") % \"mana*\"\n",
    "\n",
    "vec_query = VectorQuery(vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    num_results=10,\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    "    return_score=True,\n",
    "    filter_expression=text_filter\n",
    ")\n",
    "\n",
    "result = index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_filter =  Text(\"combined\") % \"%mana%\"\n",
    "\n",
    "vec_query = VectorQuery(vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    num_results=10,\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    "    return_score=True,\n",
    "    filter_expression=text_filter\n",
    ")\n",
    "\n",
    "result = index.query(vec_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query import RangeQuery\n",
    "\n",
    "range_query = RangeQuery(\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    "    return_score=True,\n",
    "    distance_threshold=0.8\n",
    ")\n",
    "\n",
    "result = index.query(range_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_query = RangeQuery(\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    "    distance_threshold=0.8\n",
    ")\n",
    "\n",
    "numeric_filter = Num(\"views\") >= 20\n",
    "\n",
    "range_query.set_filter(numeric_filter)\n",
    "\n",
    "# in this case we want to do a simple filter search or the vector so we execute as a joint filter directly\n",
    "result = index.query(range_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query import HybridQuery\n",
    "\n",
    "user_query = \"Manage\"\n",
    "vector = get_openai_embedding(client_secret, user_query)\n",
    "\n",
    "hybrid_query = HybridQuery(\n",
    "    text=user_query,\n",
    "    text_field_name=\"subtitle\",\n",
    "    text_scorer=\"BM25\",\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    ")\n",
    "\n",
    "result = index.query(hybrid_query)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_query = HybridQuery(\n",
    "    text=user_query,\n",
    "    text_field_name=\"subtitle\",\n",
    "    text_scorer=\"BM25\",\n",
    "    vector=np.array(vector).astype(np.float32).tobytes(),\n",
    "    vector_field_name=\"embedding\",\n",
    "    alpha=0.7, # weight the vector score lower\n",
    "    num_results=20,\n",
    "    return_fields=[\"caption\", \"subtitle\", \"combined\"],\n",
    ")\n",
    "\n",
    "result = index.query(hybrid_query)\n",
    "pd.DataFrame(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
