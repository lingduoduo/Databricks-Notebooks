{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_catalog = dbutils.widgets.get(\"ml_catalog\")\n",
    "ml_search_db = dbutils.widgets.get(\"ml_search_db\")\n",
    "df_people = spark.sql(f\"select * from {ml_catalog}.{ml_search_db}.ml_search_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Top Search People Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = spark.sql(f\"\"\"\n",
    "    SELECT _token_client_id AS client_id, \n",
    "        _id, \n",
    "        eID, \n",
    "        location, \n",
    "        position, \n",
    "        count(*) AS views\n",
    "    FROM {ml_catalog}.{ml_search_db}.ml_search_people\n",
    "    WHERE context='US'\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "    ORDER BY 6 desc\n",
    "\"\"\")\n",
    "display(df_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, round\n",
    "\n",
    "tot = df_people.agg(sum(\"views\")).collect()[0][0]\n",
    "df_people.groupBy(\"client_id\") \\\n",
    "    .agg(sum(\"views\").alias(\"total_views\")) \\\n",
    "    .withColumn(\"percent\", round(col(\"total_views\") / tot * 100, 2)) \\\n",
    "    .orderBy(\"total_views\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.groupBy(\"location\") \\\n",
    "    .agg(sum(\"views\").alias(\"total_views\")) \\\n",
    "    .withColumn(\"percent\", round(col(\"total_views\") / tot * 100, 2)) \\\n",
    "    .orderBy(\"total_views\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.groupBy(\"position\") \\\n",
    "    .agg(sum(\"views\").alias(\"total_views\")) \\\n",
    "    .withColumn(\"percent\", round(col(\"total_views\") / tot * 100, 2)) \\\n",
    "    .orderBy(\"total_views\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df_people = df_people.withColumn(\"combined\", concat_ws(\" | \", df_people[\"client_id\"], df_people[\"location\"], df_people[\"position\"]))\n",
    "df = df_people.select(\"combined\").dropDuplicates()\n",
    "pdf = df.toPandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "cert_path = parent_dir / \"seach\"/ \"utils\" / \"ADP_Internal_Root_CA_GN2.pem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from search.utils.data_profiling_llm import get_bearer_token\n",
    "\n",
    "client_secret = dbutils.widgets.get(\"client_secret\")\n",
    "bearer_token = get_bearer_token(client_secret)\n",
    "print(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.utils.data_profiling_llm import invoke_titan_model\n",
    "\n",
    "embr = invoke_titan_model(client_secret, \"testing embedding\")\n",
    "print(embr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.utils.data_profiling_llm import get_openai_embedding\n",
    "\n",
    "embr = get_openai_embedding(client_secret, \"testing embedding\")\n",
    "print(embr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secret_scope\")\n",
    "\n",
    "if secret_scope.split(\"-\")[0] == \"prod\":\n",
    "    dbutils.notebook.exit(\"No need to run the following code in Prod environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "embed_start_time = time.time()\n",
    "\n",
    "pdf = pdf.sample(frac=0.01, replace=False).reset_index(drop=True)\n",
    "pdf[\"embedding\"] = pdf.combined.apply(lambda x: get_openai_embedding(client_secret, x))\n",
    "\n",
    "embed_time = time.time() - embed_start_time\n",
    "print(f\"Embedding took {embed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = parent_dir / \"search\"/ \"data\" / \"embed.csv\"\n",
    "pdf.to_csv(data_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "yesterday = F.date_sub(F.current_date(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ml_catalog}.{ml_search_db}.ml_search_people_embed\")\n",
    "\n",
    "spark_df = spark.createDataFrame(pdf).withColumn(\"year\", F.year(F.lit(yesterday))) \\\n",
    "    .withColumn(\"month\", F.month(F.lit(yesterday))) \\\n",
    "    .withColumn(\"day\", F.day(F.lit(yesterday))) \n",
    "\n",
    "\n",
    "(spark_df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    ".partitionBy(\"year\", \"month\", \"day\")\n",
    ".saveAsTable(f\"{ml_catalog}.{ml_search_db}.ml_search_people_embed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from ast import literal_eval\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv(data_csv)\n",
    "# df[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = np.vstack(df.embedding.values)\n",
    "# print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from search.utils.data_profiling_llm import global_cluster_embeddings\n",
    "\n",
    "# reduced_embeddings = global_cluster_embeddings(matrix, dim=50)\n",
    "# print(reduced_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hdbscan import HDBSCAN\n",
    "\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=25, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "# hdbscan_model.fit(reduced_embeddings)\n",
    "# labels = hdbscan_model.labels_\n",
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in list(set(labels)):\n",
    "#     print(\"--------------------\")\n",
    "#     print(f\"Cluster {i}\")\n",
    "#     for index in np.where(labels==i)[0][:10]:\n",
    "#         print(df.iloc[index]['combined'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
